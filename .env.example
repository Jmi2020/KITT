# ==============================================================================
# KITTY Orchestrator — Environment Configuration
# ==============================================================================
# This file defines all environment variables for the KITTY system.
# Copy to .env and customize for your environment.
# Documentation: docs/configuration.md

# ==============================================================================
# 1. USER & SYSTEM BASICS
# ==============================================================================

USER_NAME=Jeremiah
PRIMARY_LOCALE=en-US

# VERBOSITY SCALE (affects all system responses)
# 1: extremely terse
# 2: concise
# 3: detailed (default)
# 4: comprehensive
# 5: exhaustive and nuanced detail
VERBOSITY=3

# System operation mode
OFFLINE_MODE=false
CONFIDENCE_THRESHOLD=0.80
BUDGET_PER_TASK_USD=0.50
ENABLE_FUNCTION_CALLING=true

# Security & Safety
API_OVERRIDE_PASSWORD=omega
HAZARD_CONFIRMATION_PHRASE=Confirm: proceed

# MQTT & Zone Configuration
TOPIC_PREFIX=kitty
ZONES=welding_bay,printer_bay,laser_room,warehouse

# ==============================================================================
# 2. LOGGING & OBSERVABILITY
# ==============================================================================

# Real-time Reasoning & Routing Logs
# Enhanced logging tracks model routing decisions and agent reasoning steps
# Set to DEBUG for detailed thinking process, INFO for key decisions only

# ML-Ready JSONL Logging (for training and analysis)
# - Lightweight: compresses 5-10x with gzip
# - ML-ready: load into pandas/PyTorch/HuggingFace datasets
# - Full structured data: prompts, responses, confidence, cost, latency
REASONING_JSONL_FILE=.logs/reasoning.jsonl
REASONING_LOG_LEVEL=DEBUG
REASONING_LOG_FILE=.logs/reasoning.log

# ==============================================================================
# 3. LLAMA.CPP PRIMARY SERVER (Single-Model Mode)
# ==============================================================================
# Use this configuration when running a single llama.cpp server
# For dual-model architecture (Q4 + F16), see section 4 below

LLAMACPP_HOST=http://host.docker.internal:8080
LLAMACPP_N_PREDICT=512
LLAMACPP_TEMPERATURE=0.1
LLAMACPP_TOP_P=0.95
LLAMACPP_REPEAT_PENALTY=1.1
LLAMACPP_TIMEOUT=1200

# Model Configuration (OPTIONAL - can use Model Manager TUI instead)
# Option 1 (Recommended): Use Model Manager TUI
#   - Start models dynamically: kitty-model-manager tui
#   - Switch models on the fly without .env changes
#
# Option 2: Auto-bootstrap from .env
#   - Configure paths below for automatic model loading
#   - Useful for automation/CI environments

LLAMACPP_MODELS_DIR=/Users/Shared/Coding/models

# Primary Model (General-purpose LLM)
LLAMACPP_PRIMARY_MODEL=llama-3-70b/Llama-3.3-70B-Instruct-F16/Llama-3.3-70B-Instruct-F16-00001-of-00004.gguf
LLAMACPP_PRIMARY_ALIAS=kitty-primary

# Coder Model (Code generation specialist)
LLAMACPP_CODER_MODEL=Qwen2.5-Coder-32B-Instruct-GGUF/qwen2.5-coder-32b-instruct-q8_0.gguf
LLAMACPP_CODER_ALIAS=kitty-coder

# Performance Tuning
LLAMACPP_CTX=8192

# GPU Optimization: Hybrid CPU+GPU load balancing for unified memory architecture
# Research shows partial offloading (30-40 layers) utilizes BOTH CPU and GPU simultaneously
# Full offload (999) leaves powerful CPU cores idle, reducing throughput by 20-40%
# Reference: Research/GPUandCPU.md
LLAMACPP_N_GPU_LAYERS=35

# Thread Configuration
# M3 Ultra has 24 P-cores - use all performance cores
# Avoid E-cores (efficiency cores) for inference workloads
LLAMACPP_THREADS=24

# Flash Attention & Batching
LLAMACPP_FLASH_ATTN=1
LLAMACPP_BATCH_SIZE=4096
LLAMACPP_UBATCH_SIZE=1024

# Parallel Streams
# Number of concurrent sequences (key lever for decode GPU usage)
# Start at 2-6, tune up to 8-12 if memory allows and GPU has headroom
LLAMACPP_PARALLEL=2

LLAMACPP_EXTRA_ARGS=

# Model Aliases (used by routing logic)
LOCAL_MODEL_PRIMARY=kitty-primary
LOCAL_MODEL_CODER=kitty-coder

# ==============================================================================
# 4. DUAL-MODEL ARCHITECTURE (ReAct Agent + Reasoning Engine)
# ==============================================================================
# Advanced configuration: Run two llama.cpp servers simultaneously
# - Q4 Model (port 8083): Fast tool orchestrator for ReAct agent workflows
# - F16 Model (port 8082): High-precision reasoning engine for complex analysis
#
# Benefits:
# - Q4 handles 90% of requests with low latency
# - F16 provides deep reasoning when delegated via reason_with_f16 tool
# - Both run in parallel for maximum throughput

# --- Q4 Tool Orchestrator (Athene V2 Agent) ---
# Optimized for: Tool calling, web search, CAD generation, device control
LLAMACPP_Q4_HOST=http://host.docker.internal:8083
LLAMACPP_Q4_ALIAS=kitty-q4
LLAMACPP_Q4_MODEL=athene-v2-agent/Athene-V2-Agent-Q4_K_M.gguf
LLAMACPP_Q4_TEMPERATURE=0.0
LLAMACPP_Q4_PORT=8083
LLAMACPP_Q4_CTX=32768
LLAMACPP_Q4_PARALLEL=4
LLAMACPP_Q4_BATCH_SIZE=4096
LLAMACPP_Q4_UBATCH_SIZE=1024

# Q4 Full GPU Offload: All 81 layers to GPU for maximum throughput
# CPU handles tokenization, sampling, KV cache, and scheduler tasks
LLAMACPP_Q4_N_GPU_LAYERS=999
LLAMACPP_Q4_THREADS=20
LLAMACPP_Q4_FLASH_ATTN=1

# --- F16 Reasoning Engine (Llama 3.3 70B) ---
# Optimized for: Comprehensive analysis, nuanced responses, complex questions
# Delegated to via the reason_with_f16 tool from Q4
LLAMACPP_F16_HOST=http://host.docker.internal:8082
LLAMACPP_F16_ALIAS=kitty-f16
LLAMACPP_F16_MODEL=llama-3-70b/Llama-3.3-70B-Instruct-F16/Llama-3.3-70B-Instruct-F16-00001-of-00004.gguf
LLAMACPP_F16_TEMPERATURE=0.2
LLAMACPP_F16_PORT=8082
LLAMACPP_F16_CTX=65536
LLAMACPP_F16_PARALLEL=6
LLAMACPP_F16_BATCH_SIZE=8192
LLAMACPP_F16_UBATCH_SIZE=2048

# F16 Full GPU Offload: All 81 layers to GPU for maximum performance
# Larger batches and parallelism keep GPU saturated during decode
LLAMACPP_F16_N_GPU_LAYERS=999
LLAMACPP_F16_THREADS=24
LLAMACPP_F16_FLASH_ATTN=1

# ==============================================================================
# 5. VISION MODEL (Multimodal Understanding)
# ==============================================================================
# Gemma 3 27B with mmproj for image understanding
# Used for vision-related queries and CAD reference image analysis

LLAMACPP_VISION_ENABLED=1
LLAMACPP_VISION_MODEL=gemma-3-27b-it-GGUF/gemma-3-27b-it-q4_k_m.gguf
LLAMACPP_VISION_MMPROJ=gemma-3-27b-it-GGUF/mmproj-google_gemma-3-27b-it-bf16.gguf
LLAMACPP_VISION_ALIAS=kitty-vision
LLAMACPP_VISION_PORT=8086
LLAMACPP_VISION_CTX=8192
LLAMACPP_VISION_PARALLEL=2
LLAMACPP_VISION_TEMPERATURE=0.0
LLAMACPP_VISION_BATCH_SIZE=1024
LLAMACPP_VISION_UBATCH_SIZE=256
LLAMACPP_VISION_N_GPU_LAYERS=999
LLAMACPP_VISION_THREADS=16
LLAMACPP_VISION_FLASH_ATTN=1

# ==============================================================================
# 6. API KEYS & EXTERNAL SERVICES
# ==============================================================================

# Cloud LLM Providers (for routing escalation)
OPENAI_API_KEY=***
ANTHROPIC_API_KEY=***
GOOGLE_API_KEY=***
PERPLEXITY_API_KEY=***

# Perplexity Model Selection
# Choose models based on research depth and budget constraints
# Pricing per 1M tokens (as of 2025-01): https://docs.perplexity.ai/guides/pricing
#
# Model Options:
#   sonar:              Fast, general search - $0.20/1M tokens (input/output combined)
#   sonar-pro:          Deep research, multi-step reasoning - $3/1M input, $15/1M output
#   sonar-reasoning:    Chain-of-thought logic - $1/1M input, $5/1M output
#   sonar-reasoning-pro: CoT + DeepSeek R1 reasoning - $2/1M input, $8/1M output
#
# Autonomous system defaults to 'sonar' for cost efficiency
# High-impact goals (score >80) may benefit from 'sonar-pro' for comprehensive research
PERPLEXITY_MODEL_SEARCH=sonar
PERPLEXITY_MODEL_REASONING=sonar-reasoning-pro
PERPLEXITY_MODEL_RESEARCH=sonar-pro

# CAD Generation Services
# Zoo.dev KittyCAD API for parametric CAD generation
ZOO_API_BASE=https://api.zoo.dev
ZOO_API_KEY=***

# Tripo AI for image-to-mesh generation
TRIPO_API_BASE=https://api.tripo.ai
TRIPO_API_KEY=***

# Web Search Backends
SEARXNG_BASE_URL=http://localhost:8888
BRAVE_SEARCH_API_KEY=
BRAVE_SEARCH_ENDPOINT=https://api.search.brave.com/res/v1/search
JINA_API_KEY=
JINA_READER_BASE_URL=https://r.jina.ai
JINA_READER_DISABLED=false

# Home Assistant Integration
# Auto-discover: python ops/scripts/discover-homeassistant.py --update-env
# Manual: Set the base URL and generate a long-lived access token from HA
HOME_ASSISTANT_BASE_URL=http://homeassistant.local:8123
HOME_ASSISTANT_URL=http://homeassistant.local:8123
HOME_ASSISTANT_TOKEN=

# Optional: Enable auto-discovery on startup
HOME_ASSISTANT_AUTO_DISCOVER=false
HOME_ASSISTANT_DISCOVERY_TIMEOUT=5.0

# ==============================================================================
# 7. STORAGE & INFRASTRUCTURE
# ==============================================================================

# Artifact Storage
# Directory for STL, STEP, OBJ files and reference images
# Accessible from macOS Finder for easy access to generated files
KITTY_ARTIFACTS_DIR=/Users/Shared/KITTY/artifacts

# MinIO (S3-compatible object store)
# Leave MINIO_ACCESS_KEY and MINIO_SECRET_KEY empty to use local filesystem fallback
MINIO_ENDPOINT=http://minio:9000
MINIO_BUCKET=kitty-artifacts
MINIO_ACCESS_KEY=
MINIO_SECRET_KEY=

# Object Store & Database URLs
OBJECT_STORE=file:///srv/kitty/artifacts
VECTOR_DB=http://vector-db:6333
BRAIN_API=http://brain-api:8080
TOOL_ROUTER="http://tool-router:8080"

# MQTT Event Bus
MQTT_URL=mqtt://event-bus:1883

# ==============================================================================
# 8. AGENT & TOOL CONFIGURATION
# ==============================================================================

# ReAct Agent Settings
# Enable agentic mode with tool calling (uses ReAct loop for multi-step workflows)
AGENTIC_MODE_ENABLED=false

# Maximum iterations for ReAct agent reasoning loop
REACT_MAX_ITERATIONS=10

# LangGraph Enhanced Routing
# Enable LangGraph state machine for structured multi-agent reasoning
# Uses Q4 fast router (80% queries) + F16 deep reasoner (20% complex queries)
# Benefits: 60% cost reduction, 40% latency improvement, better tool orchestration
BRAIN_USE_LANGGRAPH=true

# Gradual rollout percentage (0-100) for A/B testing
# Hash-based on conversation_id for consistent routing per user
# Set to 0 for no rollout, 100 for full rollout
BRAIN_LANGGRAPH_ROLLOUT_PERCENT=100

# Multi-Server llama.cpp Configuration
LLAMACPP_Q4_ALIAS=kitty-q4
LLAMACPP_F16_ALIAS=kitty-f16

# Diversity Seat Configuration (Phase 3: Diversity Seat)
# Optional: Second Q4-class model from different family (Mistral, Gemma, Phi-3)
# Reduces correlated failures in council patterns, improves proposal diversity
# Falls back to Q4 if not configured or server unavailable
LLAMACPP_Q4B_BASE=http://host.docker.internal:8084
LLAMACPP_Q4B_ALIAS=kitty-q4b
LLAMACPP_Q4B_MODEL=Mistral-7B-Instruct-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf
LLAMACPP_Q4B_PORT=8084
LLAMACPP_Q4B_CTX=8192
LLAMACPP_Q4B_PARALLEL=2
LLAMACPP_Q4B_N_GPU_LAYERS=999
LLAMACPP_Q4B_THREADS=16
LLAMACPP_Q4B_FLASH_ATTN=1

# Agent History Configuration
# Number of recent steps to include in context (0 = all steps)
AGENT_HISTORY_STEPS=4

# Maximum tokens for history context
AGENT_HISTORY_TOKEN_BUDGET=8000

# Maximum characters for tool observation summaries
AGENT_OBSERVATION_CHARS=2000

# Tool Registry Configuration
# Path to YAML file defining tool metadata, safety levels, and permissions
TOOL_REGISTRY_PATH=config/tool_registry.yaml

# ==================================================
# Quality-First Mode Configuration
# ==================================================
# Priority: Best possible answers over speed
# Target: Personal assistant, design workflows, research
# See docs/QUALITY_FIRST_MODE.md for full details

# Extended Timeout Configuration
BRAIN_REQUEST_TIMEOUT=600            # 10 minutes for brain requests
LLAMACPP_Q4_TIMEOUT=180              # 3 minutes for Q4 (fast model)
LLAMACPP_F16_TIMEOUT=600             # 10 minutes for F16 (deep reasoning)
TOOL_EXECUTION_TIMEOUT=300           # 5 minutes per tool
CAD_GENERATION_TIMEOUT=600           # 10 minutes for CAD generation
FABRICATION_ANALYSIS_TIMEOUT=180    # 3 minutes for STL analysis
WEB_SEARCH_TIMEOUT=120               # 2 minutes for research queries

# Deep Reasoning Configuration
MAX_REASONING_STEPS=10               # Up to 10 chain-of-thought steps (default: 5)
MAX_TOOL_RETRIES=5                   # Retry tools up to 5 times (default: 2)
ENABLE_TOOL_REFINEMENT=true          # Re-execute tools with refined params on F16
ENABLE_SELF_EVALUATION=true          # Quality scoring for all F16 responses
ESCALATION_COMPLEXITY_THRESHOLD=0.60 # Escalate if complexity > 0.60 (default: 0.70)

# Memory & Context Configuration
ENABLE_DEEP_SEARCH=true              # Always attempt deep memory search
MEMORY_INITIAL_THRESHOLD=0.70        # Initial search threshold (default: 0.75)
MEMORY_DEEP_THRESHOLD=0.50           # Deep search threshold (default: 0.60)
MEMORY_INITIAL_LIMIT=5               # Retrieve 5 memories initially (default: 3)
MEMORY_DEEP_LIMIT=10                 # Retrieve 10 on deep search (default: 5)
MEMORY_SUFFICIENCY_THRESHOLD=0.60    # Lower = more likely to trigger deep search (default: 0.70)

# Tool Orchestration
MAX_PARALLEL_TOOLS=3                 # Max 3 tools concurrently (default: 3)
TOOL_PRIORITY_RETRY_CRITICAL=3       # CRITICAL tools: 3 retries (default: 2)
TOOL_PRIORITY_RETRY_HIGH=3           # HIGH tools: 3 retries (default: 2)
TOOL_PRIORITY_RETRY_MEDIUM=2         # MEDIUM tools: 2 retries (default: 1)
TOOL_EXPONENTIAL_BACKOFF=true        # Use exponential backoff for retries

# Complexity Analysis Weights
COMPLEXITY_WEIGHT_TOKEN_COUNT=0.10   # Token count weight (default: 0.15)
COMPLEXITY_WEIGHT_TECHNICAL_DENSITY=0.35  # Technical density weight (default: 0.30)
COMPLEXITY_WEIGHT_MULTI_STEP=0.30    # Multi-step weight (default: 0.25)
COMPLEXITY_WEIGHT_AMBIGUITY=0.15     # Ambiguity weight (default: 0.15)
COMPLEXITY_WEIGHT_TOOL_COUNT=0.10    # Tool count weight (default: 0.15)

# Parallel Requests (reduce for single-user quality mode)
LLAMACPP_Q4_N_PARALLEL_QUALITY=2     # Only 2 concurrent Q4 requests (default: 4)
LLAMACPP_F16_N_PARALLEL_QUALITY=1    # Only 1 concurrent F16 request (default: 4)

# Monitoring & Logging
LOG_LEVEL=DEBUG                      # Detailed logging
ENABLE_ROUTING_AUDIT=true            # Log all routing decisions to DB
ENABLE_COMPLEXITY_LOGGING=true       # Log complexity scores
ENABLE_TOOL_EXECUTION_LOGGING=true   # Log all tool calls
ENABLE_LANGGRAPH_METRICS=true        # Collect LangGraph metrics
PROMETHEUS_METRICS_PORT=8000         # Expose on /metrics

# Quality Metrics
ENABLE_SELF_EVALUATION_METRICS=true  # Track F16 self-eval scores
MIN_ACCEPTABLE_CONFIDENCE=0.80       # Warn if confidence < 0.80
MIN_ACCEPTABLE_QUALITY=0.85          # Warn if self-eval < 0.85

# Adaptive Timeout (Future Feature)
ENABLE_ADAPTIVE_TIMEOUT=false        # Set to true when implemented
ADAPTIVE_TIMEOUT_CHECK_INTERVAL=30   # Check inference status every 30s
ADAPTIVE_TIMEOUT_CPU_THRESHOLD=30    # Consider "active" if CPU > 30%

# MCP Tool Servers
# URLs for Model Context Protocol tool servers
CAD_SERVICE_URL=http://cad:8200
MEM0_MCP_URL=http://mem0-mcp:8765
MAX_MEMORY_MB=20480  # Memory limit for mem0-mcp service (20GB)
BROKER_URL=http://broker:8777

# Memory Service Configuration (Phase 2: Memory Enhancement)
# Use BAAI/bge-small-en-v1.5 for better embedding quality (15-20% improvement over all-MiniLM)
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
EMBEDDING_DIM=384

# Optional reranker for improved top-k precision (20-30% improvement in relevance)
# Set to BAAI/bge-reranker-base to enable, leave empty to disable (falls back to vector-only search)
RERANKER_MODEL=BAAI/bge-reranker-base

# Collective Meta-Agent Confidentiality
# Controls memory scoping to prevent groupthink and maintain proposal independence
COLLECTIVE_PROPOSER_BLIND=1                     # Hide meta/dev context from proposers (1=blind, 0=show all)
MEMORY_EXCLUDE_TAGS=meta,dev,collective         # Tags excluded from proposer context retrieval
MEMORY_INCLUDE_TAGS=domain,procedure,safety     # Tags preferred for proposer context (empty=no preference)
COLLECTIVE_HINT_PROPOSER=Solve independently; do not reference other agents or a group.
COLLECTIVE_HINT_JUDGE=You are the judge; prefer safety, clarity, testability.

# Collective Meta-Agent Providers (Default: OFF - Zero Overhead)
# Enable cloud LLM providers in council/debate patterns for diverse opinions
# Lazy initialization: Only loads when enabled, automatic fallback to local Q4
ENABLE_OPENAI_COLLECTIVE=false
ENABLE_ANTHROPIC_COLLECTIVE=false
ENABLE_MISTRAL_COLLECTIVE=false
ENABLE_PERPLEXITY_COLLECTIVE=false
ENABLE_GEMINI_COLLECTIVE=false

# API Permission Management
# Auto-approve API calls to cloud tier (for autonomous workflows)
API_AUTO_APPROVE=false

# ==============================================================================
# 9. SECURITY & AUTHENTICATION
# ==============================================================================

# Admin Authentication (bcrypt hashes recommended)
# Generate with: python -c "from common.security import hash_password; print(hash_password('changeme'))"
ADMIN_USERS=admin:$2b$12$DUMMYPLACEHOLDER0123456789abcdefghijklmnopqrstu

# Optional: Shared voice prompt/system instructions
VOICE_SYSTEM_PROMPT="KITTY default system prompt goes here"

# ==============================================================================
# 10. SERVICE-SPECIFIC CONFIGURATION
# ==============================================================================

# --- CAD Service ---
# Local mesh generation scripts
LOCAL_MESH_SCRIPT=/opt/mesh/generate.sh
FREECAD_CMD=freecad

# --- Images Service (Stable Diffusion) ---
# Enable automatic startup with start-kitty.sh
# Set to true after downloading SD models and configuring models.yaml
IMAGES_SERVICE_ENABLED=false

# Base URL for Stable Diffusion generation
IMAGES_BASE=http://host.docker.internal:8089

# Local path to Stable Diffusion model registry
MODELS_YAML=/Users/Shared/Coding/models/models.yaml

# Redis queue + MinIO storage for SD
REDIS_URL=redis://127.0.0.1:6379/0
S3_ENDPOINT_URL=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_REGION=us-east-1
S3_BUCKET=kitty-artifacts
S3_PREFIX=images/

# Engine: diffusers (default), invokeai, a1111
IMAGE_ENGINE=diffusers

# --- Fabrication Service (Multi-Printer Control) ---
# Base URL for fabrication service (Docker internal)
FABRICATION_BASE=http://fabrication:8300

# Printer configuration (see config/printers.yaml.example)
PRINTER_CONFIG=config/printers.yaml

# Bamboo Labs H2D - High quality FDM printer (250×250×250mm)
# Access code: Settings → Network → WiFi Settings on printer display
BAMBOO_IP=192.168.1.100
BAMBOO_SERIAL=01P45165616
BAMBOO_ACCESS_CODE=your_16_char_code
BAMBOO_MQTT_HOST=192.168.1.100
BAMBOO_MQTT_PORT=1883

# Bamboo build volume (millimeters)
H2D_BUILD_WIDTH=325
H2D_BUILD_DEPTH=320
H2D_BUILD_HEIGHT=325

# Elegoo OrangeStorm Giga - Large format FDM printer (800×800×1000mm)
# Klipper with Moonraker REST API
ELEGOO_IP=192.168.1.200
ELEGOO_MOONRAKER_PORT=7125

# Elegoo build volume (millimeters)
ORANGESTORM_GIGA_BUILD_WIDTH=800
ORANGESTORM_GIGA_BUILD_DEPTH=800
ORANGESTORM_GIGA_BUILD_HEIGHT=1000

# Snapmaker Artisan Pro - 3-in-1 platform (3D/CNC/Laser, 400×400×400mm)
# SACP protocol (Snapmaker Application Communication Protocol)
SNAPMAKER_IP=192.168.1.150
SNAPMAKER_PORT=8888
SNAPMAKER_TOKEN=

# --- Phase 4: Print Monitoring & Intelligence ---
# Feature flags for external device dependencies (camera, storage, feedback)
# Disable during development, enable individually for testing

# Print Outcome Tracking
# Captures print results with visual evidence and human feedback
ENABLE_PRINT_OUTCOME_TRACKING=true

# Camera Integration
# Captures snapshots from Bamboo Labs (MQTT) and Raspberry Pi cameras (HTTP)
ENABLE_CAMERA_CAPTURE=false              # Master switch for all camera features
ENABLE_BAMBOO_CAMERA=false               # Bamboo Labs built-in camera via MQTT
ENABLE_RASPBERRY_PI_CAMERAS=false        # Raspberry Pi cameras via HTTP

# Raspberry Pi camera endpoints (HTTP snapshot servers)
SNAPMAKER_CAMERA_URL=http://snapmaker-pi.local:8080/snapshot.jpg
ELEGOO_CAMERA_URL=http://elegoo-pi.local:8080/snapshot.jpg

# Snapshot Configuration
CAMERA_SNAPSHOT_INTERVAL_MINUTES=5       # Periodic progress snapshots during prints
CAMERA_FIRST_LAYER_SNAPSHOT_DELAY=5      # Minutes after start to capture first layer

# MinIO Storage for Visual Evidence
# When disabled, snapshots URLs will be mock paths (no actual upload)
ENABLE_MINIO_SNAPSHOT_UPLOAD=false

# Human Feedback via MQTT
# Publishes review requests to UI when prints complete
ENABLE_HUMAN_FEEDBACK_REQUESTS=true      # MQTT notifications for review
HUMAN_FEEDBACK_AUTO_REQUEST=true         # Auto-request feedback after every print

# Print Intelligence (Success Prediction)
# Requires historical outcome data, disable until sufficient data collected
ENABLE_PRINT_INTELLIGENCE=false          # Success prediction and recommendations
PRINT_INTELLIGENCE_MIN_SAMPLES=30        # Minimum outcomes before predictions enabled

# --- Network Discovery Service (IoT Device Detection) ---
# Automatically discovers printers, Raspberry Pi, ESP32, and other devices
# See specs/003-NetworkDiscovery/ for details

# Discovery service port
DISCOVERY_PORT=8500

# Base URL for discovery service (Docker internal)
DISCOVERY_BASE=http://discovery:8500

# Periodic scanning configuration
# Interval in minutes for fast scans (mDNS, SSDP, UDP broadcasts)
DISCOVERY_SCAN_INTERVAL_MINUTES=15

# Interval in minutes for ICMP ping sweeps (more comprehensive, slower)
# Ping sweeps scan entire subnet(s) to find all responsive devices
DISCOVERY_PING_SWEEP_INTERVAL_MINUTES=60

# Enable/disable automatic periodic scanning
# When true, runs background scans to track IP changes and new devices
DISCOVERY_ENABLE_PERIODIC_SCANS=true

# Scanner enablement (individual protocols)
DISCOVERY_ENABLE_MDNS=true           # Bonjour/Zeroconf service discovery
DISCOVERY_ENABLE_SSDP=true           # UPnP/SSDP device discovery
DISCOVERY_ENABLE_BAMBOO_UDP=true     # Bamboo Labs printer broadcast (port 2021)
DISCOVERY_ENABLE_SNAPMAKER_UDP=true  # Snapmaker machine broadcast (port 20054)
DISCOVERY_ENABLE_NETWORK_SCAN=false  # ICMP ping sweep (requires NET_RAW capability)

# Network subnets to scan (comma-separated CIDRs)
# Example: DISCOVERY_SUBNETS=["192.168.1.0/24","10.0.0.0/24"]
# Default: ["192.168.1.0/24"]
DISCOVERY_SUBNETS=["192.168.1.0/24"]

# Discovery timeout (seconds) for manual scans
DISCOVERY_TIMEOUT_SECONDS=30

# ==============================================================================
# 11. NOTIFICATIONS
# ==============================================================================

# Enable/disable notification channels
NOTIFICATIONS_ENABLED=true
NOTIFICATIONS_SMS_ENABLED=false
NOTIFICATIONS_MACOS_ENABLED=true
NOTIFICATIONS_IMESSAGE_ENABLED=false
NOTIFICATIONS_NTFY_ENABLED=false
NOTIFICATIONS_PUSHOVER_ENABLED=false
NOTIFICATIONS_PUSHCUT_ENABLED=false

# Twilio SMS (optional)
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_FROM_NUMBER=
TWILIO_TO_NUMBER=

# ntfy.sh - Free push notifications (https://ntfy.sh/)
NTFY_TOPIC=
NTFY_SERVER=https://ntfy.sh
NTFY_PRIORITY=default
NTFY_TAGS=robot,tools

# Pushover - Reliable push notifications ($5 one-time per platform)
PUSHOVER_USER_KEY=
PUSHOVER_API_TOKEN=

# Pushcut - iOS automation and notifications
PUSHCUT_SECRET=
PUSHCUT_NOTIFICATION_NAME=

# iMessage - Built-in, syncs to all Apple devices
IMESSAGE_RECIPIENT=your.email@icloud.com

# ==============================================================================
# 12. AUTONOMOUS OPERATIONS
# ==============================================================================
# Phase 1 - Foundation: Enable bounded autonomy for KITTY to identify goals
# and execute projects during idle periods. Research work proceeds autonomously;
# fabrication requires approval.

AUTONOMOUS_ENABLED=false
AUTONOMOUS_DAILY_BUDGET_USD=5.00
AUTONOMOUS_IDLE_THRESHOLD_MINUTES=120
AUTONOMOUS_CPU_THRESHOLD_PERCENT=20.0
AUTONOMOUS_MEMORY_THRESHOLD_PERCENT=70.0

# Autonomous execution mode:
# - false (default): Only runs during 4am-6am PST window (development mode)
# - true: Runs 24/7 when system is idle for 2+ hours (production mode)
AUTONOMOUS_FULL_TIME_MODE=false

AUTONOMOUS_USER_ID=system-autonomous

# Phase 3: Outcome Tracking & Learning - Measure goal effectiveness and improve
# future goal generation through feedback loops. Tracks ROI, adoption, and impact.
OUTCOME_MEASUREMENT_ENABLED=true
OUTCOME_MEASUREMENT_WINDOW_DAYS=30
FEEDBACK_LOOP_ENABLED=true
FEEDBACK_LOOP_MIN_SAMPLES=10
FEEDBACK_LOOP_ADJUSTMENT_MAX=1.5

# ==============================================================================
# 13. CODER AGENT (LangGraph-based Code Generation)
# ==============================================================================
# LLM-powered code generation with Plan-Code-Test-Run-Refine-Summarize workflow
# Uses local llama.cpp servers for offline code generation with test-driven refinement

# Service port
CODER_PORT=8092

# Workflow parameters
CODER_MAX_REFINEMENTS=2
CODER_TIMEOUT_SECONDS=20

# llama.cpp server endpoints (Q4 for planning, F16 for precise code generation)
# Use host.docker.internal to access llama-server running on Mac host
LLAMACPP_Q4_BASE=http://host.docker.internal:8083
LLAMACPP_F16_BASE=http://host.docker.internal:8082

# Optional: Dedicated coder model endpoint (leave empty to use F16 for coding)
# If you have a dedicated coding model like DeepSeek-Coder or Qwen2.5-Coder
LLAMACPP_CODER_BASE=

# ==============================================================================
# 14. DATABASE CLUSTERING & HIGH AVAILABILITY (P2 #14)
# ==============================================================================
# PostgreSQL and Redis clustering for high availability and automatic failover
#
# To enable database clustering:
#   docker compose -f docker-compose.yml -f docker-compose.db-cluster.yml up -d
#
# Architecture:
#   PostgreSQL: 1 primary + 2 read replicas with streaming replication
#   Redis: 1 master + 2 replicas + 3 Sentinel nodes
#
# Benefits:
#   - Automatic failover if primary/master fails
#   - Read load distribution across replicas
#   - Zero downtime for read operations during failover
#   - Data durability with synchronous replication

# --- PostgreSQL High Availability ---

# PostgreSQL password (shared across all nodes)
POSTGRES_PASSWORD=changeme

# Replication user credentials
POSTGRES_REPLICATION_PASSWORD=replicator_password

# Synchronous commit mode (data safety vs. performance trade-off)
# - on: Wait for commit confirmation from synchronous replicas (safest, slower)
# - remote_write: Wait for replica to write to OS (balanced)
# - local: Only wait for local commit (faster, less safe)
# - off: No waiting (fastest, least safe - NOT recommended for production)
POSTGRES_SYNC_COMMIT=on

# Number of replicas that must acknowledge before commit completes
# - 0: Asynchronous replication (faster, risk of data loss on failover)
# - 1: One replica must confirm (balanced - recommended)
# - 2: Both replicas must confirm (safest, slower)
POSTGRES_NUM_SYNC_REPLICAS=1

# PgBouncer connection pooling (reduces database connection overhead)
# Enabled automatically with database clustering
# Max client connections: 1000
# Default pool size: 25 per database
# Pool mode: transaction (best for web applications)

# --- Redis High Availability (Sentinel) ---

# Redis password (leave empty for development, set for production)
REDIS_PASSWORD=

# Redis Sentinel password (for Sentinel-to-Sentinel communication)
REDIS_SENTINEL_PASSWORD=

# Sentinel configuration
# Down-after-milliseconds: How long before Sentinel considers master down (default: 5000ms)
# Failover-timeout: Maximum time for failover process (default: 10000ms)
# Quorum: Minimum Sentinels that must agree to elect new master (default: 2)
#
# Note: With 3 Sentinel nodes, quorum=2 means majority agreement required
#
# Sentinel will automatically:
#   1. Detect master failure after 5 seconds of no response
#   2. Coordinate with other Sentinels (quorum=2 must agree)
#   3. Promote one replica to new master
#   4. Reconfigure other replicas to follow new master
#   5. Update clients with new master address

# --- Database Clustering Ports ---
# PostgreSQL:
#   Primary: 5432
#   Replica 1: 5433
#   Replica 2: 5434
#   PgBouncer: 6432

# Redis:
#   Master: 6379
#   Replica 1: 6380
#   Replica 2: 6381
#   Sentinel 1: 26379
#   Sentinel 2: 26380
#   Sentinel 3: 26381

# --- Monitoring & Health Checks ---
# Prometheus metrics:
#   - PostgreSQL: Native metrics + connection pool stats
#   - Redis: redis-exporter on port 9121
#   - Health checks run every 10s with 5s timeout
#
# Grafana dashboards:
#   - PostgreSQL Replication Lag
#   - Redis Sentinel Status
#   - Database Connection Pool Stats
#
# HAProxy stats dashboard:
#   http://localhost:8404 (username: admin, password: HAPROXY_STATS_PASSWORD)

# ==============================================================================
# 15. MESSAGE QUEUE & ASYNC EVENT BUS (P2 #15)
# ==============================================================================
# RabbitMQ message broker for asynchronous task distribution, event sourcing,
# and decoupled service communication
#
# To enable message queue:
#   docker compose -f docker-compose.yml -f docker-compose.message-queue.yml up -d
#
# Features:
#   - Event Bus: Pub/Sub messaging for service events
#   - Task Queues: Work distribution with priority and retries
#   - RPC: Request/Reply pattern for synchronous calls
#   - Dead Letter Queues: Failed message handling
#   - Monitoring: Management UI + Prometheus metrics

# --- RabbitMQ Connection ---

# RabbitMQ credentials
RABBITMQ_USER=kitty
RABBITMQ_PASSWORD=changeme

# Virtual host (namespace isolation)
RABBITMQ_VHOST=/

# Erlang cookie for clustering (change in production!)
RABBITMQ_ERLANG_COOKIE=kitty_secret_cookie_change_me

# --- RabbitMQ Connection URL ---
# Format: amqp://user:password@host:port/vhost
# Docker internal: amqp://kitty:changeme@rabbitmq:5672/
# Host machine: amqp://kitty:changeme@localhost:5672/
RABBITMQ_URL=amqp://kitty:changeme@rabbitmq:5672/

# --- Messaging Patterns ---

# Event Bus (Pub/Sub)
# - Exchange: kitty.events (topic exchange)
# - Routing keys: service.entity.action (e.g., "fabrication.print.started")
# - Queues: Auto-created per subscriber
# - Use case: Service-to-service event notifications

# Task Queues (Work Distribution)
# - Exchange: kitty.tasks (direct exchange)
# - Queues: research.tasks, cad.tasks, brain.commands
# - Priority: 0-10 (higher = more urgent)
# - Retries: 3 attempts with exponential backoff
# - Use case: Background job processing

# RPC (Request/Reply)
# - Exchange: kitty.rpc (direct exchange)
# - Pattern: Temporary reply queue per client
# - Timeout: 30 seconds default
# - Use case: Synchronous inter-service calls

# Dead Letter Exchange (Failed Messages)
# - Exchange: dlx (topic exchange)
# - Queue: dlx.queue (captures all failed messages)
# - Retention: Indefinite (manual review required)
# - Use case: Error analysis and recovery

# --- Management UI & Monitoring ---

# RabbitMQ Management UI
# URL: http://localhost:15672
# Login: RABBITMQ_USER / RABBITMQ_PASSWORD
# Features:
#   - Queue/exchange management
#   - Message browsing and publishing
#   - Connection monitoring
#   - Performance metrics

# Prometheus Metrics
# URL: http://localhost:15692/metrics
# Metrics include:
#   - Message rates (publish/deliver/ack)
#   - Queue lengths and consumers
#   - Connection and channel counts
#   - Memory and disk usage

# --- Message Queue Ports ---
# AMQP Protocol: 5672
# Management UI: 15672
# Prometheus Metrics: 15692

# --- Queue Policies ---
# High Availability: All queues replicated across nodes (future clustering)
# Dead Letter Exchange: Automatic routing of failed messages to dlx
# TTL: Retry queues expire after 5 minutes of inactivity
# Max Length: fabrication.events=100k, research.tasks=10k, cad.tasks=5k

# --- Performance Tuning ---
# Memory high watermark: 60% of available RAM
# Disk free limit: 2GB minimum
# Channel max: 2048 channels per connection
# Prefetch count: 1 (fair dispatch to workers)
# Heartbeat: 60 seconds

# --- Usage Examples ---
# Python Event Bus:
#   from common.messaging import EventBus
#   bus = EventBus(RABBITMQ_URL, source="my-service")
#   bus.publish("fabrication.print.started", {"job_id": "123"})
#
# Python Task Queue:
#   from common.messaging import TaskQueue
#   queue = TaskQueue(RABBITMQ_URL, "research.tasks")
#   task_id = queue.submit("research_paper", {"query": "3D printing"})
#
# Python RPC:
#   from common.messaging import RPCClient
#   client = RPCClient(RABBITMQ_URL)
#   result = client.call("rpc_queue", {"operation": "add", "a": 5, "b": 3})
