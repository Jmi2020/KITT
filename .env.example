# ==============================================================================
# KITTY Orchestrator — Environment Configuration
# ==============================================================================
# This file defines all environment variables for the KITTY system.
# Copy to .env and customize for your environment.
# Documentation: docs/configuration.md

# ==============================================================================
# 1. USER & SYSTEM BASICS
# ==============================================================================

USER_NAME=Jeremiah
PRIMARY_LOCALE=en-US

# VERBOSITY SCALE (affects all system responses)
# 1: extremely terse
# 2: concise
# 3: detailed (default)
# 4: comprehensive
# 5: exhaustive and nuanced detail
VERBOSITY=3

# System operation mode
OFFLINE_MODE=true
CONFIDENCE_THRESHOLD=0.80
BUDGET_PER_TASK_USD=0.50

# Security & Safety
API_OVERRIDE_PASSWORD=omega
HAZARD_CONFIRMATION_PHRASE=Confirm: proceed

# MQTT & Zone Configuration
TOPIC_PREFIX=kitty
ZONES=welding_bay,printer_bay,laser_room,warehouse

# ==============================================================================
# 2. LOGGING & OBSERVABILITY
# ==============================================================================

# Real-time Reasoning & Routing Logs
# Enhanced logging tracks model routing decisions and agent reasoning steps
# Set to DEBUG for detailed thinking process, INFO for key decisions only

# ML-Ready JSONL Logging (for training and analysis)
# - Lightweight: compresses 5-10x with gzip
# - ML-ready: load into pandas/PyTorch/HuggingFace datasets
# - Full structured data: prompts, responses, confidence, cost, latency
REASONING_JSONL_FILE=.logs/reasoning.jsonl
REASONING_LOG_LEVEL=DEBUG
REASONING_LOG_FILE=.logs/reasoning.log

# ==============================================================================
# 3. LLAMA.CPP PRIMARY SERVER (Single-Model Mode)
# ==============================================================================
# Use this configuration when running a single llama.cpp server
# For dual-model architecture (Q4 + F16), see section 4 below

LLAMACPP_HOST=http://host.docker.internal:8080
LLAMACPP_N_PREDICT=512
LLAMACPP_TEMPERATURE=0.1
LLAMACPP_TOP_P=0.95
LLAMACPP_REPEAT_PENALTY=1.1
LLAMACPP_TIMEOUT=1200

# Model Configuration (OPTIONAL - can use Model Manager TUI instead)
# Option 1 (Recommended): Use Model Manager TUI
#   - Start models dynamically: kitty-model-manager tui
#   - Switch models on the fly without .env changes
#
# Option 2: Auto-bootstrap from .env
#   - Configure paths below for automatic model loading
#   - Useful for automation/CI environments

LLAMACPP_MODELS_DIR=/Users/Shared/Coding/models

# Primary Model (General-purpose LLM)
LLAMACPP_PRIMARY_MODEL=llama-3-70b/Llama-3.3-70B-Instruct-F16/Llama-3.3-70B-Instruct-F16-00001-of-00004.gguf
LLAMACPP_PRIMARY_ALIAS=kitty-primary

# Coder Model (Code generation specialist)
LLAMACPP_CODER_MODEL=Qwen2.5-Coder-32B-Instruct-GGUF/qwen2.5-coder-32b-instruct-q8_0.gguf
LLAMACPP_CODER_ALIAS=kitty-coder

# Performance Tuning
LLAMACPP_CTX=8192

# GPU Optimization: Hybrid CPU+GPU load balancing for unified memory architecture
# Research shows partial offloading (30-40 layers) utilizes BOTH CPU and GPU simultaneously
# Full offload (999) leaves powerful CPU cores idle, reducing throughput by 20-40%
# Reference: Research/GPUandCPU.md
LLAMACPP_N_GPU_LAYERS=35

# Thread Configuration
# M3 Ultra has 24 P-cores - use all performance cores
# Avoid E-cores (efficiency cores) for inference workloads
LLAMACPP_THREADS=24

# Flash Attention & Batching
LLAMACPP_FLASH_ATTN=1
LLAMACPP_BATCH_SIZE=4096
LLAMACPP_UBATCH_SIZE=1024

# Parallel Streams
# Number of concurrent sequences (key lever for decode GPU usage)
# Start at 2-6, tune up to 8-12 if memory allows and GPU has headroom
LLAMACPP_PARALLEL=2

LLAMACPP_EXTRA_ARGS=

# Model Aliases (used by routing logic)
LOCAL_MODEL_PRIMARY=kitty-primary
LOCAL_MODEL_CODER=kitty-coder

# ==============================================================================
# 4. DUAL-MODEL ARCHITECTURE (ReAct Agent + Reasoning Engine)
# ==============================================================================
# Advanced configuration: Run two llama.cpp servers simultaneously
# - Q4 Model (port 8083): Fast tool orchestrator for ReAct agent workflows
# - F16 Model (port 8082): High-precision reasoning engine for complex analysis
#
# Benefits:
# - Q4 handles 90% of requests with low latency
# - F16 provides deep reasoning when delegated via reason_with_f16 tool
# - Both run in parallel for maximum throughput

# --- Q4 Tool Orchestrator (Athene V2 Agent) ---
# Optimized for: Tool calling, web search, CAD generation, device control
LLAMACPP_Q4_HOST=http://host.docker.internal:8083
LLAMACPP_Q4_ALIAS=kitty-q4
LLAMACPP_Q4_MODEL=athene-v2-agent/Athene-V2-Agent-Q4_K_M.gguf
LLAMACPP_Q4_TEMPERATURE=0.0
LLAMACPP_Q4_PORT=8083
LLAMACPP_Q4_CTX=32768
LLAMACPP_Q4_PARALLEL=4
LLAMACPP_Q4_BATCH_SIZE=4096
LLAMACPP_Q4_UBATCH_SIZE=1024

# Q4 Full GPU Offload: All 81 layers to GPU for maximum throughput
# CPU handles tokenization, sampling, KV cache, and scheduler tasks
LLAMACPP_Q4_N_GPU_LAYERS=999
LLAMACPP_Q4_THREADS=20
LLAMACPP_Q4_FLASH_ATTN=1

# --- F16 Reasoning Engine (Llama 3.3 70B) ---
# Optimized for: Comprehensive analysis, nuanced responses, complex questions
# Delegated to via the reason_with_f16 tool from Q4
LLAMACPP_F16_HOST=http://host.docker.internal:8082
LLAMACPP_F16_ALIAS=kitty-f16
LLAMACPP_F16_MODEL=llama-3-70b/Llama-3.3-70B-Instruct-F16/Llama-3.3-70B-Instruct-F16-00001-of-00004.gguf
LLAMACPP_F16_TEMPERATURE=0.2
LLAMACPP_F16_PORT=8082
LLAMACPP_F16_CTX=65536
LLAMACPP_F16_PARALLEL=4
LLAMACPP_F16_BATCH_SIZE=4096
LLAMACPP_F16_UBATCH_SIZE=1024

# F16 Full GPU Offload: All 81 layers to GPU for maximum performance
# Larger batches and parallelism keep GPU saturated during decode
LLAMACPP_F16_N_GPU_LAYERS=999
LLAMACPP_F16_THREADS=20
LLAMACPP_F16_FLASH_ATTN=1

# ==============================================================================
# 5. VISION MODEL (Multimodal Understanding)
# ==============================================================================
# Gemma 3 27B with mmproj for image understanding
# Used for vision-related queries and CAD reference image analysis

LLAMACPP_VISION_ENABLED=1
LLAMACPP_VISION_MODEL=gemma-3-27b-it-GGUF/gemma-3-27b-it-q4_k_m.gguf
LLAMACPP_VISION_MMPROJ=gemma-3-27b-it-GGUF/mmproj-google_gemma-3-27b-it-bf16.gguf
LLAMACPP_VISION_ALIAS=kitty-vision
LLAMACPP_VISION_PORT=8086
LLAMACPP_VISION_CTX=8192
LLAMACPP_VISION_PARALLEL=2
LLAMACPP_VISION_TEMPERATURE=0.0
LLAMACPP_VISION_BATCH_SIZE=1024
LLAMACPP_VISION_UBATCH_SIZE=256
LLAMACPP_VISION_N_GPU_LAYERS=999
LLAMACPP_VISION_THREADS=16
LLAMACPP_VISION_FLASH_ATTN=1

# ==============================================================================
# 6. API KEYS & EXTERNAL SERVICES
# ==============================================================================

# Cloud LLM Providers (for routing escalation)
OPENAI_API_KEY=***
ANTHROPIC_API_KEY=***
GOOGLE_API_KEY=***
PERPLEXITY_API_KEY=***

# CAD Generation Services
# Zoo.dev KittyCAD API for parametric CAD generation
ZOO_API_BASE=https://api.zoo.dev
ZOO_API_KEY=***

# Tripo AI for image-to-mesh generation
TRIPO_API_BASE=https://api.tripo.ai
TRIPO_API_KEY=***

# Web Search Backends
SEARXNG_BASE_URL=http://localhost:8888
BRAVE_SEARCH_API_KEY=
BRAVE_SEARCH_ENDPOINT=https://api.search.brave.com/res/v1/search
JINA_API_KEY=
JINA_READER_BASE_URL=https://r.jina.ai
JINA_READER_DISABLED=false

# Home Assistant Integration
# Auto-discover: python ops/scripts/discover-homeassistant.py --update-env
# Manual: Set the base URL and generate a long-lived access token from HA
HOME_ASSISTANT_BASE_URL=http://homeassistant.local:8123
HOME_ASSISTANT_URL=http://homeassistant.local:8123
HOME_ASSISTANT_TOKEN=

# Optional: Enable auto-discovery on startup
HOME_ASSISTANT_AUTO_DISCOVER=false
HOME_ASSISTANT_DISCOVERY_TIMEOUT=5.0

# ==============================================================================
# 7. STORAGE & INFRASTRUCTURE
# ==============================================================================

# Artifact Storage
# Directory for STL, STEP, OBJ files and reference images
# Accessible from macOS Finder for easy access to generated files
KITTY_ARTIFACTS_DIR=/Users/Shared/KITTY/artifacts

# MinIO (S3-compatible object store)
# Leave MINIO_ACCESS_KEY and MINIO_SECRET_KEY empty to use local filesystem fallback
MINIO_ENDPOINT=http://minio:9000
MINIO_BUCKET=kitty-artifacts
MINIO_ACCESS_KEY=
MINIO_SECRET_KEY=

# Object Store & Database URLs
OBJECT_STORE=file:///srv/kitty/artifacts
VECTOR_DB=http://vector-db:6333
BRAIN_API=http://brain-api:8080
TOOL_ROUTER=http://tool-router:8080

# MQTT Event Bus
MQTT_URL=mqtt://event-bus:1883

# ==============================================================================
# 8. AGENT & TOOL CONFIGURATION
# ==============================================================================

# ReAct Agent Settings
# Enable agentic mode with tool calling (uses ReAct loop for multi-step workflows)
AGENTIC_MODE_ENABLED=false

# Maximum iterations for ReAct agent reasoning loop
REACT_MAX_ITERATIONS=10

# LangGraph Enhanced Routing
# Enable LangGraph state machine for structured multi-agent reasoning
# Uses Q4 fast router (80% queries) + F16 deep reasoner (20% complex queries)
# Benefits: 60% cost reduction, 40% latency improvement, better tool orchestration
BRAIN_USE_LANGGRAPH=true

# Gradual rollout percentage (0-100) for A/B testing
# Hash-based on conversation_id for consistent routing per user
# Set to 0 for no rollout, 100 for full rollout
BRAIN_LANGGRAPH_ROLLOUT_PERCENT=100

# Multi-Server llama.cpp Configuration
LLAMACPP_Q4_ALIAS=kitty-q4
LLAMACPP_F16_ALIAS=kitty-f16

# Agent History Configuration
# Number of recent steps to include in context (0 = all steps)
AGENT_HISTORY_STEPS=4

# Maximum tokens for history context
AGENT_HISTORY_TOKEN_BUDGET=8000

# Maximum characters for tool observation summaries
AGENT_OBSERVATION_CHARS=2000

# Tool Registry Configuration
# Path to YAML file defining tool metadata, safety levels, and permissions
TOOL_REGISTRY_PATH=config/tool_registry.yaml

# ==================================================
# Quality-First Mode Configuration
# ==================================================
# Priority: Best possible answers over speed
# Target: Personal assistant, design workflows, research
# See docs/QUALITY_FIRST_MODE.md for full details

# Extended Timeout Configuration
BRAIN_REQUEST_TIMEOUT=600            # 10 minutes for brain requests
LLAMACPP_Q4_TIMEOUT=180              # 3 minutes for Q4 (fast model)
LLAMACPP_F16_TIMEOUT=600             # 10 minutes for F16 (deep reasoning)
TOOL_EXECUTION_TIMEOUT=300           # 5 minutes per tool
CAD_GENERATION_TIMEOUT=600           # 10 minutes for CAD generation
FABRICATION_ANALYSIS_TIMEOUT=180    # 3 minutes for STL analysis
WEB_SEARCH_TIMEOUT=120               # 2 minutes for research queries

# Deep Reasoning Configuration
MAX_REASONING_STEPS=10               # Up to 10 chain-of-thought steps (default: 5)
MAX_TOOL_RETRIES=5                   # Retry tools up to 5 times (default: 2)
ENABLE_TOOL_REFINEMENT=true          # Re-execute tools with refined params on F16
ENABLE_SELF_EVALUATION=true          # Quality scoring for all F16 responses
ESCALATION_COMPLEXITY_THRESHOLD=0.60 # Escalate if complexity > 0.60 (default: 0.70)

# Memory & Context Configuration
ENABLE_DEEP_SEARCH=true              # Always attempt deep memory search
MEMORY_INITIAL_THRESHOLD=0.70        # Initial search threshold (default: 0.75)
MEMORY_DEEP_THRESHOLD=0.50           # Deep search threshold (default: 0.60)
MEMORY_INITIAL_LIMIT=5               # Retrieve 5 memories initially (default: 3)
MEMORY_DEEP_LIMIT=10                 # Retrieve 10 on deep search (default: 5)
MEMORY_SUFFICIENCY_THRESHOLD=0.60    # Lower = more likely to trigger deep search (default: 0.70)

# Tool Orchestration
MAX_PARALLEL_TOOLS=3                 # Max 3 tools concurrently (default: 3)
TOOL_PRIORITY_RETRY_CRITICAL=3       # CRITICAL tools: 3 retries (default: 2)
TOOL_PRIORITY_RETRY_HIGH=3           # HIGH tools: 3 retries (default: 2)
TOOL_PRIORITY_RETRY_MEDIUM=2         # MEDIUM tools: 2 retries (default: 1)
TOOL_EXPONENTIAL_BACKOFF=true        # Use exponential backoff for retries

# Complexity Analysis Weights
COMPLEXITY_WEIGHT_TOKEN_COUNT=0.10   # Token count weight (default: 0.15)
COMPLEXITY_WEIGHT_TECHNICAL_DENSITY=0.35  # Technical density weight (default: 0.30)
COMPLEXITY_WEIGHT_MULTI_STEP=0.30    # Multi-step weight (default: 0.25)
COMPLEXITY_WEIGHT_AMBIGUITY=0.15     # Ambiguity weight (default: 0.15)
COMPLEXITY_WEIGHT_TOOL_COUNT=0.10    # Tool count weight (default: 0.15)

# Parallel Requests (reduce for single-user quality mode)
LLAMACPP_Q4_N_PARALLEL_QUALITY=2     # Only 2 concurrent Q4 requests (default: 4)
LLAMACPP_F16_N_PARALLEL_QUALITY=1    # Only 1 concurrent F16 request (default: 4)

# Monitoring & Logging
LOG_LEVEL=DEBUG                      # Detailed logging
ENABLE_ROUTING_AUDIT=true            # Log all routing decisions to DB
ENABLE_COMPLEXITY_LOGGING=true       # Log complexity scores
ENABLE_TOOL_EXECUTION_LOGGING=true   # Log all tool calls
ENABLE_LANGGRAPH_METRICS=true        # Collect LangGraph metrics
PROMETHEUS_METRICS_PORT=8000         # Expose on /metrics

# Quality Metrics
ENABLE_SELF_EVALUATION_METRICS=true  # Track F16 self-eval scores
MIN_ACCEPTABLE_CONFIDENCE=0.80       # Warn if confidence < 0.80
MIN_ACCEPTABLE_QUALITY=0.85          # Warn if self-eval < 0.85

# Adaptive Timeout (Future Feature)
ENABLE_ADAPTIVE_TIMEOUT=false        # Set to true when implemented
ADAPTIVE_TIMEOUT_CHECK_INTERVAL=30   # Check inference status every 30s
ADAPTIVE_TIMEOUT_CPU_THRESHOLD=30    # Consider "active" if CPU > 30%

# MCP Tool Servers
# URLs for Model Context Protocol tool servers
CAD_SERVICE_URL=http://cad:8200
MEM0_MCP_URL=http://mem0-mcp:8765
MAX_MEMORY_MB=20480  # Memory limit for mem0-mcp service (20GB)
BROKER_URL=http://broker:8777

# Memory Service Configuration (Phase 2: Memory Enhancement)
# Use BAAI/bge-small-en-v1.5 for better embedding quality (15-20% improvement over all-MiniLM)
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
EMBEDDING_DIM=384

# Optional reranker for improved top-k precision (20-30% improvement in relevance)
# Set to BAAI/bge-reranker-base to enable, leave empty to disable (falls back to vector-only search)
RERANKER_MODEL=BAAI/bge-reranker-base

# Collective Meta-Agent Confidentiality
# Controls memory scoping to prevent groupthink and maintain proposal independence
COLLECTIVE_PROPOSER_BLIND=1                     # Hide meta/dev context from proposers (1=blind, 0=show all)
MEMORY_EXCLUDE_TAGS=meta,dev,collective         # Tags excluded from proposer context retrieval
MEMORY_INCLUDE_TAGS=domain,procedure,safety     # Tags preferred for proposer context (empty=no preference)
COLLECTIVE_HINT_PROPOSER=Solve independently; do not reference other agents or a group.
COLLECTIVE_HINT_JUDGE=You are the judge; prefer safety, clarity, testability.

# API Permission Management
# Auto-approve API calls to cloud tier (for autonomous workflows)
API_AUTO_APPROVE=false

# ==============================================================================
# 9. SECURITY & AUTHENTICATION
# ==============================================================================

# Admin Authentication (bcrypt hashes recommended)
# Generate with: python -c "from common.security import hash_password; print(hash_password('changeme'))"
ADMIN_USERS=admin:$2b$12$DUMMYPLACEHOLDER0123456789abcdefghijklmnopqrstu

# Optional: Shared voice prompt/system instructions
VOICE_SYSTEM_PROMPT="KITTY default system prompt goes here"

# ==============================================================================
# 10. SERVICE-SPECIFIC CONFIGURATION
# ==============================================================================

# --- CAD Service ---
# Local mesh generation scripts
LOCAL_MESH_SCRIPT=/opt/mesh/generate.sh
FREECAD_CMD=freecad

# --- Images Service (Stable Diffusion) ---
# Enable automatic startup with start-kitty.sh
# Set to true after downloading SD models and configuring models.yaml
IMAGES_SERVICE_ENABLED=false

# Base URL for Stable Diffusion generation
IMAGES_BASE=http://host.docker.internal:8089

# Local path to Stable Diffusion model registry
MODELS_YAML=/Users/Shared/Coding/models/models.yaml

# Redis queue + MinIO storage for SD
REDIS_URL=redis://127.0.0.1:6379/0
S3_ENDPOINT_URL=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_REGION=us-east-1
S3_BUCKET=kitty-artifacts
S3_PREFIX=images/

# Engine: diffusers (default), invokeai, a1111
IMAGE_ENGINE=diffusers

# --- Vision Gallery UI ---
VISION_GALLERY_PORT=5173
VISION_GALLERY_API_BASE=http://localhost:8080
VISION_GALLERY_MQTT_URL=ws://localhost:9001

# --- Fabrication Service (Multi-Printer Control) ---
# Base URL for fabrication service (Docker internal)
FABRICATION_BASE=http://fabrication:8300

# Printer configuration (see config/printers.yaml.example)
PRINTER_CONFIG=config/printers.yaml

# Bamboo Labs H2D - High quality FDM printer (250×250×250mm)
# Access code: Settings → Network → WiFi Settings on printer display
BAMBOO_IP=192.168.1.100
BAMBOO_SERIAL=01P45165616
BAMBOO_ACCESS_CODE=your_16_char_code
BAMBOO_MQTT_HOST=192.168.1.100
BAMBOO_MQTT_PORT=1883

# Bamboo build volume (millimeters)
H2D_BUILD_WIDTH=325
H2D_BUILD_DEPTH=320
H2D_BUILD_HEIGHT=325

# Elegoo OrangeStorm Giga - Large format FDM printer (800×800×1000mm)
# Klipper with Moonraker REST API
ELEGOO_IP=192.168.1.200
ELEGOO_MOONRAKER_PORT=7125

# Elegoo build volume (millimeters)
ORANGESTORM_GIGA_BUILD_WIDTH=800
ORANGESTORM_GIGA_BUILD_DEPTH=800
ORANGESTORM_GIGA_BUILD_HEIGHT=1000

# Snapmaker Artisan Pro - 3-in-1 platform (3D/CNC/Laser, 400×400×400mm)
# SACP protocol (Snapmaker Application Communication Protocol)
SNAPMAKER_IP=192.168.1.150
SNAPMAKER_PORT=8888
SNAPMAKER_TOKEN=

# --- Network Discovery Service (IoT Device Detection) ---
# Automatically discovers printers, Raspberry Pi, ESP32, and other devices
# See specs/003-NetworkDiscovery/ for details

# Discovery service port
DISCOVERY_PORT=8500

# Base URL for discovery service (Docker internal)
DISCOVERY_BASE=http://discovery:8500

# Periodic scanning configuration
# Interval in minutes for fast scans (mDNS, SSDP, UDP broadcasts)
DISCOVERY_SCAN_INTERVAL_MINUTES=15

# Interval in minutes for ICMP ping sweeps (more comprehensive, slower)
# Ping sweeps scan entire subnet(s) to find all responsive devices
DISCOVERY_PING_SWEEP_INTERVAL_MINUTES=60

# Enable/disable automatic periodic scanning
# When true, runs background scans to track IP changes and new devices
DISCOVERY_ENABLE_PERIODIC_SCANS=true

# Scanner enablement (individual protocols)
DISCOVERY_ENABLE_MDNS=true           # Bonjour/Zeroconf service discovery
DISCOVERY_ENABLE_SSDP=true           # UPnP/SSDP device discovery
DISCOVERY_ENABLE_BAMBOO_UDP=true     # Bamboo Labs printer broadcast (port 2021)
DISCOVERY_ENABLE_SNAPMAKER_UDP=true  # Snapmaker machine broadcast (port 20054)
DISCOVERY_ENABLE_NETWORK_SCAN=false  # ICMP ping sweep (requires NET_RAW capability)

# Network subnets to scan (comma-separated CIDRs)
# Example: DISCOVERY_SUBNETS=["192.168.1.0/24","10.0.0.0/24"]
# Default: ["192.168.1.0/24"]
DISCOVERY_SUBNETS=["192.168.1.0/24"]

# Discovery timeout (seconds) for manual scans
DISCOVERY_TIMEOUT_SECONDS=30

# ==============================================================================
# 11. NOTIFICATIONS
# ==============================================================================

# Enable/disable notification channels
NOTIFICATIONS_ENABLED=true
NOTIFICATIONS_SMS_ENABLED=false
NOTIFICATIONS_MACOS_ENABLED=true
NOTIFICATIONS_IMESSAGE_ENABLED=false
NOTIFICATIONS_NTFY_ENABLED=false
NOTIFICATIONS_PUSHOVER_ENABLED=false
NOTIFICATIONS_PUSHCUT_ENABLED=false

# Twilio SMS (optional)
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_FROM_NUMBER=
TWILIO_TO_NUMBER=

# ntfy.sh - Free push notifications (https://ntfy.sh/)
NTFY_TOPIC=
NTFY_SERVER=https://ntfy.sh
NTFY_PRIORITY=default
NTFY_TAGS=robot,tools

# Pushover - Reliable push notifications ($5 one-time per platform)
PUSHOVER_USER_KEY=
PUSHOVER_API_TOKEN=

# Pushcut - iOS automation and notifications
PUSHCUT_SECRET=
PUSHCUT_NOTIFICATION_NAME=

# iMessage - Built-in, syncs to all Apple devices
IMESSAGE_RECIPIENT=your.email@icloud.com

# ==============================================================================
# 12. AUTONOMOUS OPERATIONS
# ==============================================================================
# Phase 1 - Foundation: Enable bounded autonomy for KITTY to identify goals
# and execute projects during idle periods. Research work proceeds autonomously;
# fabrication requires approval.

AUTONOMOUS_ENABLED=false
AUTONOMOUS_DAILY_BUDGET_USD=5.00
AUTONOMOUS_IDLE_THRESHOLD_MINUTES=120
AUTONOMOUS_CPU_THRESHOLD_PERCENT=20.0
AUTONOMOUS_MEMORY_THRESHOLD_PERCENT=70.0

# ==============================================================================
# 13. CODER AGENT (LangGraph-based Code Generation)
# ==============================================================================
# LLM-powered code generation with Plan-Code-Test-Run-Refine-Summarize workflow
# Uses local llama.cpp servers for offline code generation with test-driven refinement

# Service port
CODER_PORT=8092

# Workflow parameters
CODER_MAX_REFINEMENTS=2
CODER_TIMEOUT_SECONDS=20

# llama.cpp server endpoints (Q4 for planning, F16 for precise code generation)
# Use host.docker.internal to access llama-server running on Mac host
LLAMACPP_Q4_BASE=http://host.docker.internal:8083
LLAMACPP_F16_BASE=http://host.docker.internal:8082

# Optional: Dedicated coder model endpoint (leave empty to use F16 for coding)
# If you have a dedicated coding model like DeepSeek-Coder or Qwen2.5-Coder
LLAMACPP_CODER_BASE=
AUTONOMOUS_USER_ID=system-autonomous
