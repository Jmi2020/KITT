# ==============================================================================
# KITTY Orchestrator — Environment Configuration
# ==============================================================================
# This file defines all environment variables for the KITTY system.
# Copy to .env and customize for your environment.
# Documentation: docs/configuration.md

# ==============================================================================
# 1. USER & SYSTEM BASICS
# ==============================================================================

USER_NAME=Jeremiah
PRIMARY_LOCALE=en-US

# VERBOSITY SCALE (affects all system responses)
# 1: extremely terse
# 2: concise
# 3: detailed (default)
# 4: comprehensive
# 5: exhaustive and nuanced detail
VERBOSITY=3

# System operation mode
OFFLINE_MODE=true
CONFIDENCE_THRESHOLD=0.80
BUDGET_PER_TASK_USD=0.50

# Security & Safety
API_OVERRIDE_PASSWORD=omega
HAZARD_CONFIRMATION_PHRASE=Confirm: proceed

# MQTT & Zone Configuration
TOPIC_PREFIX=kitty
ZONES=welding_bay,printer_bay,laser_room,warehouse

# ==============================================================================
# 2. LOGGING & OBSERVABILITY
# ==============================================================================

# Real-time Reasoning & Routing Logs
# Enhanced logging tracks model routing decisions and agent reasoning steps
# Set to DEBUG for detailed thinking process, INFO for key decisions only

# ML-Ready JSONL Logging (for training and analysis)
# - Lightweight: compresses 5-10x with gzip
# - ML-ready: load into pandas/PyTorch/HuggingFace datasets
# - Full structured data: prompts, responses, confidence, cost, latency
REASONING_JSONL_FILE=.logs/reasoning.jsonl
REASONING_LOG_LEVEL=DEBUG
REASONING_LOG_FILE=.logs/reasoning.log

# ==============================================================================
# 3. LLAMA.CPP PRIMARY SERVER (Single-Model Mode)
# ==============================================================================
# Use this configuration when running a single llama.cpp server
# For dual-model architecture (Q4 + F16), see section 4 below

LLAMACPP_HOST=http://host.docker.internal:8080
LLAMACPP_N_PREDICT=512
LLAMACPP_TEMPERATURE=0.1
LLAMACPP_TOP_P=0.95
LLAMACPP_REPEAT_PENALTY=1.1
LLAMACPP_TIMEOUT=1200

# Model Configuration (OPTIONAL - can use Model Manager TUI instead)
# Option 1 (Recommended): Use Model Manager TUI
#   - Start models dynamically: kitty-model-manager tui
#   - Switch models on the fly without .env changes
#
# Option 2: Auto-bootstrap from .env
#   - Configure paths below for automatic model loading
#   - Useful for automation/CI environments

LLAMACPP_MODELS_DIR=/Users/Shared/Coding/models

# Primary Model (General-purpose LLM)
LLAMACPP_PRIMARY_MODEL=llama-3-70b/Llama-3.3-70B-Instruct-F16/Llama-3.3-70B-Instruct-F16-00001-of-00004.gguf
LLAMACPP_PRIMARY_ALIAS=kitty-primary

# Coder Model (Code generation specialist)
LLAMACPP_CODER_MODEL=Qwen2.5-Coder-32B-Instruct-GGUF/qwen2.5-coder-32b-instruct-q8_0.gguf
LLAMACPP_CODER_ALIAS=kitty-coder

# Performance Tuning
LLAMACPP_CTX=8192

# GPU Optimization: Hybrid CPU+GPU load balancing for unified memory architecture
# Research shows partial offloading (30-40 layers) utilizes BOTH CPU and GPU simultaneously
# Full offload (999) leaves powerful CPU cores idle, reducing throughput by 20-40%
# Reference: Research/GPUandCPU.md
LLAMACPP_N_GPU_LAYERS=35

# Thread Configuration
# M3 Ultra has 24 P-cores - use all performance cores
# Avoid E-cores (efficiency cores) for inference workloads
LLAMACPP_THREADS=24

# Flash Attention & Batching
LLAMACPP_FLASH_ATTN=1
LLAMACPP_BATCH_SIZE=4096
LLAMACPP_UBATCH_SIZE=1024

# Parallel Streams
# Number of concurrent sequences (key lever for decode GPU usage)
# Start at 2-6, tune up to 8-12 if memory allows and GPU has headroom
LLAMACPP_PARALLEL=2

LLAMACPP_EXTRA_ARGS=

# Model Aliases (used by routing logic)
LOCAL_MODEL_PRIMARY=kitty-primary
LOCAL_MODEL_CODER=kitty-coder

# ==============================================================================
# 4. DUAL-MODEL ARCHITECTURE (ReAct Agent + Reasoning Engine)
# ==============================================================================
# Advanced configuration: Run two llama.cpp servers simultaneously
# - Q4 Model (port 8083): Fast tool orchestrator for ReAct agent workflows
# - F16 Model (port 8082): High-precision reasoning engine for complex analysis
#
# Benefits:
# - Q4 handles 90% of requests with low latency
# - F16 provides deep reasoning when delegated via reason_with_f16 tool
# - Both run in parallel for maximum throughput

# --- Q4 Tool Orchestrator (Athene V2 Agent) ---
# Optimized for: Tool calling, web search, CAD generation, device control
LLAMACPP_Q4_HOST=http://host.docker.internal:8083
LLAMACPP_Q4_ALIAS=kitty-q4
LLAMACPP_Q4_MODEL=athene-v2-agent/Athene-V2-Agent-Q4_K_M.gguf
LLAMACPP_Q4_TEMPERATURE=0.0
LLAMACPP_Q4_PORT=8083
LLAMACPP_Q4_CTX=32768
LLAMACPP_Q4_PARALLEL=4
LLAMACPP_Q4_BATCH_SIZE=4096
LLAMACPP_Q4_UBATCH_SIZE=1024

# Q4 Full GPU Offload: All 81 layers to GPU for maximum throughput
# CPU handles tokenization, sampling, KV cache, and scheduler tasks
LLAMACPP_Q4_N_GPU_LAYERS=999
LLAMACPP_Q4_THREADS=20
LLAMACPP_Q4_FLASH_ATTN=1

# --- F16 Reasoning Engine (Llama 3.3 70B) ---
# Optimized for: Comprehensive analysis, nuanced responses, complex questions
# Delegated to via the reason_with_f16 tool from Q4
LLAMACPP_F16_HOST=http://host.docker.internal:8082
LLAMACPP_F16_ALIAS=kitty-f16
LLAMACPP_F16_MODEL=llama-3-70b/Llama-3.3-70B-Instruct-F16/Llama-3.3-70B-Instruct-F16-00001-of-00004.gguf
LLAMACPP_F16_TEMPERATURE=0.2
LLAMACPP_F16_PORT=8082
LLAMACPP_F16_CTX=65536
LLAMACPP_F16_PARALLEL=4
LLAMACPP_F16_BATCH_SIZE=4096
LLAMACPP_F16_UBATCH_SIZE=1024

# F16 Full GPU Offload: All 81 layers to GPU for maximum performance
# Larger batches and parallelism keep GPU saturated during decode
LLAMACPP_F16_N_GPU_LAYERS=999
LLAMACPP_F16_THREADS=20
LLAMACPP_F16_FLASH_ATTN=1

# ==============================================================================
# 5. VISION MODEL (Multimodal Understanding)
# ==============================================================================
# Gemma 3 27B with mmproj for image understanding
# Used for vision-related queries and CAD reference image analysis

LLAMACPP_VISION_ENABLED=1
LLAMACPP_VISION_MODEL=gemma-3-27b-it-GGUF/gemma-3-27b-it-q4_k_m.gguf
LLAMACPP_VISION_MMPROJ=gemma-3-27b-it-GGUF/mmproj-google_gemma-3-27b-it-bf16.gguf
LLAMACPP_VISION_ALIAS=kitty-vision
LLAMACPP_VISION_PORT=8086
LLAMACPP_VISION_CTX=8192
LLAMACPP_VISION_PARALLEL=2
LLAMACPP_VISION_TEMPERATURE=0.0
LLAMACPP_VISION_BATCH_SIZE=1024
LLAMACPP_VISION_UBATCH_SIZE=256
LLAMACPP_VISION_N_GPU_LAYERS=999
LLAMACPP_VISION_THREADS=16
LLAMACPP_VISION_FLASH_ATTN=1

# ==============================================================================
# 6. API KEYS & EXTERNAL SERVICES
# ==============================================================================

# Cloud LLM Providers (for routing escalation)
OPENAI_API_KEY=***
ANTHROPIC_API_KEY=***
GOOGLE_API_KEY=***
PERPLEXITY_API_KEY=***

# CAD Generation Services
# Zoo.dev KittyCAD API for parametric CAD generation
ZOO_API_BASE=https://api.zoo.dev
ZOO_API_KEY=***

# Tripo AI for image-to-mesh generation
TRIPO_API_BASE=https://api.tripo.ai
TRIPO_API_KEY=***

# Web Search Backends
SEARXNG_BASE_URL=http://localhost:8888
BRAVE_SEARCH_API_KEY=
BRAVE_SEARCH_ENDPOINT=https://api.search.brave.com/res/v1/search
JINA_API_KEY=
JINA_READER_BASE_URL=https://r.jina.ai
JINA_READER_DISABLED=false

# Home Assistant Integration
# Auto-discover: python ops/scripts/discover-homeassistant.py --update-env
# Manual: Set the base URL and generate a long-lived access token from HA
HOME_ASSISTANT_BASE_URL=http://homeassistant.local:8123
HOME_ASSISTANT_URL=http://homeassistant.local:8123
HOME_ASSISTANT_TOKEN=

# Optional: Enable auto-discovery on startup
HOME_ASSISTANT_AUTO_DISCOVER=false
HOME_ASSISTANT_DISCOVERY_TIMEOUT=5.0

# ==============================================================================
# 7. STORAGE & INFRASTRUCTURE
# ==============================================================================

# Artifact Storage
# Directory for STL, STEP, OBJ files and reference images
# Accessible from macOS Finder for easy access to generated files
KITTY_ARTIFACTS_DIR=/Users/Shared/KITTY/artifacts

# MinIO (S3-compatible object store)
# Leave MINIO_ACCESS_KEY and MINIO_SECRET_KEY empty to use local filesystem fallback
MINIO_ENDPOINT=http://minio:9000
MINIO_BUCKET=kitty-artifacts
MINIO_ACCESS_KEY=
MINIO_SECRET_KEY=

# Object Store & Database URLs
OBJECT_STORE=file:///srv/kitty/artifacts
VECTOR_DB=http://vector-db:6333
BRAIN_API=http://brain-api:8080
TOOL_ROUTER=http://tool-router:8080

# MQTT Event Bus
MQTT_URL=mqtt://event-bus:1883

# ==============================================================================
# 8. AGENT & TOOL CONFIGURATION
# ==============================================================================

# ReAct Agent Settings
# Enable agentic mode with tool calling (uses ReAct loop for multi-step workflows)
AGENTIC_MODE_ENABLED=false

# Maximum iterations for ReAct agent reasoning loop
REACT_MAX_ITERATIONS=10

# Agent History Configuration
# Number of recent steps to include in context (0 = all steps)
AGENT_HISTORY_STEPS=4

# Maximum tokens for history context
AGENT_HISTORY_TOKEN_BUDGET=8000

# Maximum characters for tool observation summaries
AGENT_OBSERVATION_CHARS=2000

# Tool Registry Configuration
# Path to YAML file defining tool metadata, safety levels, and permissions
TOOL_REGISTRY_PATH=config/tool_registry.yaml

# MCP Tool Servers
# URLs for Model Context Protocol tool servers
CAD_SERVICE_URL=http://cad:8200
MEM0_MCP_URL=http://mem0-mcp:8765
BROKER_URL=http://broker:8777

# API Permission Management
# Auto-approve API calls to cloud tier (for autonomous workflows)
API_AUTO_APPROVE=false

# ==============================================================================
# 9. SECURITY & AUTHENTICATION
# ==============================================================================

# Admin Authentication (bcrypt hashes recommended)
# Generate with: python -c "from common.security import hash_password; print(hash_password('changeme'))"
ADMIN_USERS=admin:$2b$12$DUMMYPLACEHOLDER0123456789abcdefghijklmnopqrstu

# Optional: Shared voice prompt/system instructions
VOICE_SYSTEM_PROMPT="KITTY default system prompt goes here"

# ==============================================================================
# 10. SERVICE-SPECIFIC CONFIGURATION
# ==============================================================================

# --- CAD Service ---
# Local mesh generation scripts
LOCAL_MESH_SCRIPT=/opt/mesh/generate.sh
FREECAD_CMD=freecad

# --- Images Service (Stable Diffusion) ---
# Enable automatic startup with start-kitty.sh
# Set to true after downloading SD models and configuring models.yaml
IMAGES_SERVICE_ENABLED=false

# Base URL for Stable Diffusion generation
IMAGES_BASE=http://host.docker.internal:8089

# Local path to Stable Diffusion model registry
MODELS_YAML=/Users/Shared/Coding/models/models.yaml

# Redis queue + MinIO storage for SD
REDIS_URL=redis://127.0.0.1:6379/0
S3_ENDPOINT_URL=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_REGION=us-east-1
S3_BUCKET=kitty-artifacts
S3_PREFIX=images/

# Engine: diffusers (default), invokeai, a1111
IMAGE_ENGINE=diffusers

# --- Vision Gallery UI ---
VISION_GALLERY_PORT=5173
VISION_GALLERY_API_BASE=http://localhost:8080
VISION_GALLERY_MQTT_URL=ws://localhost:9001

# --- Fabrication Service (Multi-Printer Control) ---
# Base URL for fabrication service (Docker internal)
FABRICATION_BASE=http://fabrication:8300

# Printer configuration (see config/printers.yaml.example)
PRINTER_CONFIG=config/printers.yaml

# Bamboo Labs H2D - High quality FDM printer (250×250×250mm)
# Access code: Settings → Network → WiFi Settings on printer display
BAMBOO_IP=192.168.1.100
BAMBOO_SERIAL=01P45165616
BAMBOO_ACCESS_CODE=your_16_char_code
BAMBOO_MQTT_HOST=192.168.1.100
BAMBOO_MQTT_PORT=1883

# Bamboo build volume (millimeters)
H2D_BUILD_WIDTH=325
H2D_BUILD_DEPTH=320
H2D_BUILD_HEIGHT=325

# Elegoo OrangeStorm Giga - Large format FDM printer (800×800×1000mm)
# Klipper with Moonraker REST API
ELEGOO_IP=192.168.1.200
ELEGOO_MOONRAKER_PORT=7125

# Elegoo build volume (millimeters)
ORANGESTORM_GIGA_BUILD_WIDTH=800
ORANGESTORM_GIGA_BUILD_DEPTH=800
ORANGESTORM_GIGA_BUILD_HEIGHT=1000

# Snapmaker Artisan Pro - 3-in-1 platform (3D/CNC/Laser, 400×400×400mm)
# SACP protocol (Snapmaker Application Communication Protocol)
SNAPMAKER_IP=192.168.1.150
SNAPMAKER_PORT=8888
SNAPMAKER_TOKEN=

# --- Network Discovery Service (IoT Device Detection) ---
# Automatically discovers printers, Raspberry Pi, ESP32, and other devices
# See specs/003-NetworkDiscovery/ for details

# Discovery service port
DISCOVERY_PORT=8500

# Base URL for discovery service (Docker internal)
DISCOVERY_BASE=http://discovery:8500

# Periodic scanning configuration
# Interval in minutes for fast scans (mDNS, SSDP, UDP broadcasts)
DISCOVERY_SCAN_INTERVAL_MINUTES=15

# Interval in minutes for ICMP ping sweeps (more comprehensive, slower)
# Ping sweeps scan entire subnet(s) to find all responsive devices
DISCOVERY_PING_SWEEP_INTERVAL_MINUTES=60

# Enable/disable automatic periodic scanning
# When true, runs background scans to track IP changes and new devices
DISCOVERY_ENABLE_PERIODIC_SCANS=true

# Scanner enablement (individual protocols)
DISCOVERY_ENABLE_MDNS=true           # Bonjour/Zeroconf service discovery
DISCOVERY_ENABLE_SSDP=true           # UPnP/SSDP device discovery
DISCOVERY_ENABLE_BAMBOO_UDP=true     # Bamboo Labs printer broadcast (port 2021)
DISCOVERY_ENABLE_SNAPMAKER_UDP=true  # Snapmaker machine broadcast (port 20054)
DISCOVERY_ENABLE_NETWORK_SCAN=false  # ICMP ping sweep (requires NET_RAW capability)

# Network subnets to scan (comma-separated CIDRs)
# Example: DISCOVERY_SUBNETS=["192.168.1.0/24","10.0.0.0/24"]
# Default: ["192.168.1.0/24"]
DISCOVERY_SUBNETS=["192.168.1.0/24"]

# Discovery timeout (seconds) for manual scans
DISCOVERY_TIMEOUT_SECONDS=30

# ==============================================================================
# 11. NOTIFICATIONS
# ==============================================================================

# Enable/disable notification channels
NOTIFICATIONS_ENABLED=true
NOTIFICATIONS_SMS_ENABLED=false
NOTIFICATIONS_MACOS_ENABLED=true
NOTIFICATIONS_IMESSAGE_ENABLED=false
NOTIFICATIONS_NTFY_ENABLED=false
NOTIFICATIONS_PUSHOVER_ENABLED=false
NOTIFICATIONS_PUSHCUT_ENABLED=false

# Twilio SMS (optional)
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_FROM_NUMBER=
TWILIO_TO_NUMBER=

# ntfy.sh - Free push notifications (https://ntfy.sh/)
NTFY_TOPIC=
NTFY_SERVER=https://ntfy.sh
NTFY_PRIORITY=default
NTFY_TAGS=robot,tools

# Pushover - Reliable push notifications ($5 one-time per platform)
PUSHOVER_USER_KEY=
PUSHOVER_API_TOKEN=

# Pushcut - iOS automation and notifications
PUSHCUT_SECRET=
PUSHCUT_NOTIFICATION_NAME=

# iMessage - Built-in, syncs to all Apple devices
IMESSAGE_RECIPIENT=your.email@icloud.com

# ==============================================================================
# 12. AUTONOMOUS OPERATIONS
# ==============================================================================
# Phase 1 - Foundation: Enable bounded autonomy for KITTY to identify goals
# and execute projects during idle periods. Research work proceeds autonomously;
# fabrication requires approval.

AUTONOMOUS_ENABLED=false
AUTONOMOUS_DAILY_BUDGET_USD=5.00
AUTONOMOUS_IDLE_THRESHOLD_MINUTES=120
AUTONOMOUS_CPU_THRESHOLD_PERCENT=20.0
AUTONOMOUS_MEMORY_THRESHOLD_PERCENT=70.0

# ==============================================================================
# 13. CODER AGENT (LangGraph-based Code Generation)
# ==============================================================================
# LLM-powered code generation with Plan-Code-Test-Run-Refine-Summarize workflow
# Uses local llama.cpp servers for offline code generation with test-driven refinement

# Service port
CODER_PORT=8092

# Workflow parameters
CODER_MAX_REFINEMENTS=2
CODER_TIMEOUT_SECONDS=20

# llama.cpp server endpoints (Q4 for planning, F16 for precise code generation)
# Use host.docker.internal to access llama-server running on Mac host
LLAMACPP_Q4_BASE=http://host.docker.internal:8083
LLAMACPP_F16_BASE=http://host.docker.internal:8082

# Optional: Dedicated coder model endpoint (leave empty to use F16 for coding)
# If you have a dedicated coding model like DeepSeek-Coder or Qwen2.5-Coder
LLAMACPP_CODER_BASE=
AUTONOMOUS_USER_ID=system-autonomous
