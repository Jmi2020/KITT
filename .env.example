# KITTY Orchestrator â€” .env (example)
USER_NAME=Jeremiah
PRIMARY_LOCALE=en-US
# VERBOSITY SCALE
# 1: extremely terse
# 2: concise
# 3: detailed (default)
# 4: comprehensive
# 5: exhaustive and nuanced detail
VERBOSITY=3
OFFLINE_MODE=true
CONFIDENCE_THRESHOLD=0.80
BUDGET_PER_TASK_USD=0.50
API_OVERRIDE_PASSWORD=omega
HAZARD_CONFIRMATION_PHRASE=Confirm: proceed
TOPIC_PREFIX=kitty
ZONES=welding_bay,printer_bay,laser_room,warehouse
# ==============================================================================
# Real-time Reasoning & Routing Logging
# ==============================================================================
# Enhanced logging for tracking model routing decisions and agent reasoning
# Set to DEBUG to see detailed thinking process, INFO for key decisions only
# Logs show: tier selection, confidence scores, tool use, agent steps

# ML-Ready JSONL Logging (for training and analysis)
# ==============================================================================
# JSONL format: each log entry is a single JSON object on one line
# - Lightweight: compresses 5-10x with gzip
# - ML-ready: load into pandas/PyTorch/HuggingFace datasets
# - Full structured data: prompts, responses, confidence, cost, latency, metadata
# Usage:
#   from brain.logging_config import load_jsonl_logs, analyze_routing_performance
#   logs = load_jsonl_logs(".logs/reasoning.jsonl")
#   stats = analyze_routing_performance(".logs/reasoning.jsonl")
REASONING_JSONL_FILE=.logs/reasoning.jsonl
REASONING_LOG_LEVEL=DEBUG
REASONING_LOG_FILE=.logs/reasoning.log


LLAMACPP_HOST=http://host.docker.internal:8080
LLAMACPP_N_PREDICT=512
LLAMACPP_TEMPERATURE=0.1
LLAMACPP_TOP_P=0.95
LLAMACPP_REPEAT_PENALTY=1.1
LLAMACPP_TIMEOUT=300

# ==============================================================================
# llama.cpp Model Configuration (OPTIONAL)
# ==============================================================================
# These fields are OPTIONAL. You can use KITTY in two ways:
#
# Option 1 (Recommended): Use Model Manager TUI
#   - Start models dynamically: kitty-model-manager tui
#   - Switch models on the fly
#   - No .env configuration needed
#
# Option 2: Auto-bootstrap from .env
#   - Configure paths below for automatic model loading
#   - Useful for automation/CI environments
#
# If these are empty and no server is running, you'll get a helpful error
# message with instructions.
# ==============================================================================
LLAMACPP_MODELS_DIR=/Users/Shared/Coding/models
LLAMACPP_PRIMARY_MODEL=llama-3-70b/Llama-3.3-70B-Instruct-F16/Llama-3.3-70B-Instruct-F16-00001-of-00004.gguf
LLAMACPP_PRIMARY_ALIAS=kitty-primary
LLAMACPP_CODER_MODEL=Qwen2.5-Coder-32B-Instruct-GGUF/qwen2.5-coder-32b-instruct-q8_0.gguf
LLAMACPP_CODER_ALIAS=kitty-coder
LLAMACPP_CTX=8192
# GPU Optimization: Hybrid CPU+GPU load balancing for unified memory architecture
# Research shows partial offloading (30-40 layers) utilizes BOTH CPU and GPU simultaneously
# Full offload (999) leaves powerful CPU cores idle, reducing throughput by 20-40%
# Reference: Research/GPUandCPU.md
LLAMACPP_N_GPU_LAYERS=35
# Thread count: M3 Ultra has 24 P-cores (not 20) - use all performance cores
# Avoid E-cores (efficiency cores) for inference workloads
LLAMACPP_THREADS=24
LLAMACPP_FLASH_ATTN=1
# Batch configuration for GPU occupancy and throughput
# -b (batch): Prefill batch size for prompt ingestion (larger = better GPU utilization)
# --ubatch (micro-batch): Kernel tiling size (tune for memory bandwidth)
LLAMACPP_BATCH_SIZE=4096
LLAMACPP_UBATCH_SIZE=1024
# Parallel streams: Number of concurrent sequences (key lever for decode GPU usage)
# Start at 6, tune up to 8-12 if memory allows and GPU has headroom
LLAMACPP_PARALLEL=2
LLAMACPP_EXTRA_ARGS=

# -------------------------
# Dual-Model Architecture (Q4 Tool Orchestrator + F16 Reasoning Engine)
# -------------------------
# Q4 Model (Athene V2 Agent - Port 8083)
# Handles tool calling, web search, CAD generation, device control
LLAMACPP_Q4_HOST=http://host.docker.internal:8083
LLAMACPP_Q4_ALIAS=kitty-q4
LLAMACPP_Q4_MODEL=athene-v2-agent/Athene-V2-Agent-Q4_K_M.gguf
LLAMACPP_Q4_TEMPERATURE=0.0
LLAMACPP_Q4_PORT=8083
LLAMACPP_Q4_CTX=16384
LLAMACPP_Q4_PARALLEL=4
LLAMACPP_Q4_BATCH_SIZE=4096
LLAMACPP_Q4_UBATCH_SIZE=1024
# Q4 Full GPU Offload: All 81 layers to GPU for maximum throughput
# CPU handles tokenization, sampling, KV cache, and scheduler tasks
LLAMACPP_Q4_N_GPU_LAYERS=999
LLAMACPP_Q4_THREADS=20
LLAMACPP_Q4_FLASH_ATTN=1

# F16 Model (Reasoning Engine - Port 8082)
# Provides comprehensive, nuanced analysis for complex questions
# Delegated to via the reason_with_f16 tool from Q4
LLAMACPP_F16_HOST=http://host.docker.internal:8082
LLAMACPP_F16_ALIAS=kitty-f16
LLAMACPP_F16_MODEL=llama-3-70b/Llama-3.3-70B-Instruct-F16/Llama-3.3-70B-Instruct-F16-00001-of-00004.gguf
LLAMACPP_F16_TEMPERATURE=0.2
LLAMACPP_F16_PORT=8082
LLAMACPP_F16_CTX=16384
LLAMACPP_F16_PARALLEL=4
LLAMACPP_F16_BATCH_SIZE=4096
LLAMACPP_F16_UBATCH_SIZE=1024
# F16 Full GPU Offload: All 81 layers to GPU for maximum performance
# Larger batches and parallelism keep GPU saturated during decode
LLAMACPP_F16_N_GPU_LAYERS=999
LLAMACPP_F16_THREADS=20
LLAMACPP_F16_FLASH_ATTN=1

LOCAL_MODEL_PRIMARY=kitty-primary
LOCAL_MODEL_CODER=kitty-coder

# Admin authentication (bcrypt hashes recommended)
# Generate with: python -c "from common.security import hash_password; print(hash_password('changeme'))"
ADMIN_USERS=admin:$2b$12$DUMMYPLACEHOLDER0123456789abcdefghijklmnopqrstu

# Optional: shared voice prompt/system instructions (use single-line or escaped newlines)
VOICE_SYSTEM_PROMPT="KITTY default system prompt goes here"

# Home Assistant Integration
# Auto-discover: python ops/scripts/discover-homeassistant.py --update-env
# Manual: Set the base URL and generate a long-lived access token from HA
HOME_ASSISTANT_BASE_URL=http://homeassistant.local:8123
HOME_ASSISTANT_URL=http://homeassistant.local:8123
HOME_ASSISTANT_TOKEN=
# Optional: Enable auto-discovery on startup (default: false)
HOME_ASSISTANT_AUTO_DISCOVER=false
# Optional: Discovery timeout in seconds (default: 5.0)
HOME_ASSISTANT_DISCOVERY_TIMEOUT=5.0

# MCP Tool Servers (for Agentic Workflows)
# CAD service URL for generate_cad_model tool
CAD_SERVICE_URL=http://cad:8200
# Memory service URL for semantic memory (already set via MEM0_MCP_URL)
# MEM0_MCP_URL is set in Docker Compose
# Enable agentic mode with tool calling (default: false)
AGENTIC_MODE_ENABLED=false
# Maximum iterations for ReAct agent (default: 10)
REACT_MAX_ITERATIONS=10

# API Permission Management
# Auto-approve API calls to cloud tier (for autonomous workflows, default: false)
API_AUTO_APPROVE=false
# Override password for manual API approval (default: omega)
API_OVERRIDE_PASSWORD=omega

OPENAI_API_KEY=***
ANTHROPIC_API_KEY=***
GOOGLE_API_KEY=***
PERPLEXITY_API_KEY=***

# ==============================================================================
# Artifact Storage Configuration
# ==============================================================================
# Directory for storing CAD artifacts (STL, STEP, OBJ files) and reference images
# This directory is accessible from macOS Finder for easy access to generated files
# Default: /Users/Shared/KITTY/artifacts
# The directory will be created automatically if it doesn't exist
#
# Usage:
# - Generated STL files can be opened in Fusion 360, Blender, etc.
# - Reference images are stored here for CAD generation
# - All artifacts include metadata for tracking provenance
KITTY_ARTIFACTS_DIR=/Users/Shared/KITTY/artifacts

# CAD Service Configuration
# Zoo.dev KittyCAD API for parametric CAD generation
ZOO_API_BASE=https://api.zoo.dev
ZOO_API_KEY=***
# Tripo AI for image-to-mesh generation
TRIPO_API_BASE=https://api.tripo.ai
TRIPO_API_KEY=***
# MinIO S3-compatible object store for CAD artifacts
# Leave MINIO_ACCESS_KEY and MINIO_SECRET_KEY empty to use local filesystem fallback
MINIO_ENDPOINT=http://minio:9000
MINIO_BUCKET=kitty-artifacts
MINIO_ACCESS_KEY=
MINIO_SECRET_KEY=
# Local mesh generation script (TripoSR/InstantMesh)
LOCAL_MESH_SCRIPT=/opt/mesh/generate.sh
# FreeCAD fallback command
FREECAD_CMD=freecad

# ==============================================================================
# Images Service Configuration (Stable Diffusion)
# ==============================================================================
# Enable automatic startup of images service with start-kitty.sh
# Set to true after downloading SD models and configuring models.yaml
IMAGES_SERVICE_ENABLED=false

# Base URL for the images_service (Stable Diffusion generation)
# Default: http://127.0.0.1:8089 (local service)
IMAGES_BASE=http://127.0.0.1:8089

# Local path to Stable Diffusion model registry
MODELS_YAML=/Users/Shared/Coding/models/models.yaml

# Redis queue + MinIO storage
REDIS_URL=redis://127.0.0.1:6379/0
S3_ENDPOINT_URL=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_REGION=us-east-1
S3_BUCKET=kitty-artifacts
S3_PREFIX=images/

# Engine: diffusers (default), invokeai, a1111
IMAGE_ENGINE=diffusers

OBJECT_STORE=file:///srv/kitty/artifacts
VECTOR_DB=http://vector-db:6333
BRAIN_API=http://brain-api:8080
TOOL_ROUTER=http://tool-router:8080
MQTT_URL=mqtt://event-bus:1883

# Notifications
# Twilio SMS notifications (optional)
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_FROM_NUMBER=
TWILIO_TO_NUMBER=

# ntfy.sh - Free push notifications to all devices (iOS, Android, Web)
# Create a unique topic name (e.g., kitty-jeremiah-workshop)
# Download app: https://ntfy.sh/ or use ntfy.sh web interface
NTFY_TOPIC=
NTFY_SERVER=https://ntfy.sh
NTFY_PRIORITY=default
NTFY_TAGS=robot,tools

# ==============================================================================
# Autonomous Operations (Phase 1 - Foundation)
# ==============================================================================
# Enable bounded autonomy for KITTY to identify goals and execute projects
# during idle periods. Research work proceeds autonomously; fabrication requires approval.
AUTONOMOUS_ENABLED=false
AUTONOMOUS_DAILY_BUDGET_USD=5.00
AUTONOMOUS_IDLE_THRESHOLD_MINUTES=120
AUTONOMOUS_CPU_THRESHOLD_PERCENT=20.0
AUTONOMOUS_MEMORY_THRESHOLD_PERCENT=70.0
AUTONOMOUS_USER_ID=system-autonomous

# Pushover - Reliable push notifications ($5 one-time per platform)
# Sign up at https://pushover.net
PUSHOVER_USER_KEY=
PUSHOVER_API_TOKEN=

# Pushcut - iOS automation and notifications
# Get from https://www.pushcut.io
PUSHCUT_SECRET=
PUSHCUT_NOTIFICATION_NAME=

# iMessage - Built-in, syncs to all Apple devices (no app needed!)
# Use your iCloud email or phone number
IMESSAGE_RECIPIENT=your.email@icloud.com

# Enable/disable notification channels
NOTIFICATIONS_ENABLED=true
NOTIFICATIONS_SMS_ENABLED=false
NOTIFICATIONS_MACOS_ENABLED=true
NOTIFICATIONS_IMESSAGE_ENABLED=false
NOTIFICATIONS_NTFY_ENABLED=false
NOTIFICATIONS_PUSHOVER_ENABLED=false
NOTIFICATIONS_PUSHCUT_ENABLED=false

# ==============================================================================
# Web Search Backends
# ==============================================================================
SEARXNG_BASE_URL=http://localhost:8888
BRAVE_SEARCH_API_KEY=
BRAVE_SEARCH_ENDPOINT=https://api.search.brave.com/res/v1/search
JINA_API_KEY=
JINA_READER_BASE_URL=https://r.jina.ai
JINA_READER_DISABLED=false
# Vision Gallery UI
VISION_GALLERY_PORT=5173
VISION_GALLERY_API_BASE=http://localhost:8080
VISION_GALLERY_MQTT_URL=ws://localhost:9001
