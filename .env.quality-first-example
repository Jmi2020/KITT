# ============================================================
# KITTY Brain - Quality-First Mode Configuration
# ============================================================
# Copy relevant sections to your .env file
# Priority: Best possible answers over speed
# Target: Personal assistant, design workflows, research
# ============================================================

# ===== LangGraph Multi-Agent Routing =====
BRAIN_USE_LANGGRAPH=true
BRAIN_LANGGRAPH_ROLLOUT_PERCENT=100

# ===== Timeout Configuration =====
# Allow long-running inference for quality results
BRAIN_REQUEST_TIMEOUT=600            # 10 minutes for brain requests
LLAMACPP_TIMEOUT=300                 # 5 minutes default llama.cpp timeout
LLAMACPP_Q4_TIMEOUT=180              # 3 minutes for Q4 (fast model)
LLAMACPP_F16_TIMEOUT=600             # 10 minutes for F16 (deep reasoning)

# Tool execution timeouts (generous for thorough work)
TOOL_EXECUTION_TIMEOUT=300           # 5 minutes per tool
CAD_GENERATION_TIMEOUT=600           # 10 minutes for CAD generation
FABRICATION_ANALYSIS_TIMEOUT=180    # 3 minutes for STL analysis
WEB_SEARCH_TIMEOUT=120               # 2 minutes for research queries

# ===== Deep Reasoning Configuration =====
# Allow extensive reasoning iterations
MAX_REASONING_STEPS=10               # Up to 10 chain-of-thought steps (default: 5)
MAX_TOOL_RETRIES=5                   # Retry tools up to 5 times (default: 2)
ENABLE_TOOL_REFINEMENT=true          # Re-execute tools with refined params on F16
ENABLE_SELF_EVALUATION=true          # Quality scoring for all F16 responses

# F16 escalation triggers (more aggressive routing to deep reasoner)
ESCALATION_CONFIDENCE_THRESHOLD=0.80 # Escalate if Q4 confidence < 0.80 (default: 0.75)
ESCALATION_COMPLEXITY_THRESHOLD=0.60 # Escalate if complexity > 0.60 (default: 0.70)

# ===== Memory & Context Configuration =====
# Deep memory search for comprehensive context
ENABLE_DEEP_SEARCH=true              # Always attempt deep memory search
MEMORY_INITIAL_THRESHOLD=0.70        # Initial search threshold (default: 0.75)
MEMORY_DEEP_THRESHOLD=0.50           # Deep search threshold (default: 0.60)
MEMORY_INITIAL_LIMIT=5               # Retrieve 5 memories initially (default: 3)
MEMORY_DEEP_LIMIT=10                 # Retrieve 10 on deep search (default: 5)
MEMORY_SUFFICIENCY_THRESHOLD=0.60    # Lower = more likely to trigger deep search (default: 0.70)

# ===== Tool Orchestration =====
# Parallel execution but with high retry tolerance
MAX_PARALLEL_TOOLS=3                 # Max 3 tools concurrently (default: 3)
TOOL_PRIORITY_RETRY_CRITICAL=3       # CRITICAL tools: 3 retries (default: 2)
TOOL_PRIORITY_RETRY_HIGH=3           # HIGH tools: 3 retries (default: 2)
TOOL_PRIORITY_RETRY_MEDIUM=2         # MEDIUM tools: 2 retries (default: 1)
TOOL_EXPONENTIAL_BACKOFF=true        # Use exponential backoff for retries

# ===== Complexity Analysis =====
# More sensitive complexity detection â†’ more F16 escalation
COMPLEXITY_WEIGHT_TOKEN_COUNT=0.10   # Token count weight (default: 0.15)
COMPLEXITY_WEIGHT_TECHNICAL_DENSITY=0.35  # Technical density weight (default: 0.30)
COMPLEXITY_WEIGHT_MULTI_STEP=0.30    # Multi-step weight (default: 0.25)
COMPLEXITY_WEIGHT_AMBIGUITY=0.15     # Ambiguity weight (default: 0.15)
COMPLEXITY_WEIGHT_TOOL_COUNT=0.10    # Tool count weight (default: 0.15)

# ===== llama.cpp Server Configuration =====
# Multi-server routing
LLAMACPP_Q4_HOST=http://host.docker.internal:8083
LLAMACPP_Q4_ALIAS=kitty-q4
LLAMACPP_Q4_MODEL=athene-v2-agent/Athene-V2-Agent-Q4_K_M.gguf

LLAMACPP_F16_HOST=http://host.docker.internal:8082
LLAMACPP_F16_ALIAS=kitty-f16
LLAMACPP_F16_MODEL=llama-3.3-70b/Llama-3.3-70B-Instruct-F16.gguf

# Context size (larger for quality mode)
LLAMACPP_Q4_CTX_SIZE=8192            # 8K context for Q4
LLAMACPP_F16_CTX_SIZE=16384          # 16K context for F16 (if RAM allows)

# Parallel requests (lower for single-user quality mode)
LLAMACPP_Q4_N_PARALLEL=2             # Only 2 concurrent Q4 requests (default: 4)
LLAMACPP_F16_N_PARALLEL=1            # Only 1 concurrent F16 request (default: 2)

# ===== Monitoring & Logging =====
# Verbose logging for quality analysis
LOG_LEVEL=DEBUG                      # Detailed logging
ENABLE_ROUTING_AUDIT=true            # Log all routing decisions to DB
ENABLE_COMPLEXITY_LOGGING=true       # Log complexity scores
ENABLE_TOOL_EXECUTION_LOGGING=true   # Log all tool calls

# Prometheus metrics
ENABLE_LANGGRAPH_METRICS=true        # Collect LangGraph metrics
PROMETHEUS_METRICS_PORT=8000         # Expose on /metrics

# ===== Quality Metrics =====
# Track answer quality
ENABLE_SELF_EVALUATION_METRICS=true  # Track F16 self-eval scores
MIN_ACCEPTABLE_CONFIDENCE=0.80       # Warn if confidence < 0.80
MIN_ACCEPTABLE_QUALITY=0.85          # Warn if self-eval < 0.85

# ===== Adaptive Timeout (Future Feature) =====
# Check if GPU/CPU still working before timing out
ENABLE_ADAPTIVE_TIMEOUT=false        # Set to true when implemented
ADAPTIVE_TIMEOUT_CHECK_INTERVAL=30   # Check inference status every 30s
ADAPTIVE_TIMEOUT_CPU_THRESHOLD=30    # Consider "active" if CPU > 30%

# ===== Verbosity =====
# Default verbosity for responses
VERBOSITY=5                          # Exhaustive detail (scale 1-5)

# ===== User Preferences =====
USER_NAME=YourName                   # Personalization
HAZARD_CONFIRMATION_PHRASE=Confirm: proceed

# ===== API Keys (if using external tools) =====
# Add your API keys for research tools
PERPLEXITY_API_KEY=your_key_here
ZOO_API_KEY=your_key_here
TRIPO_API_KEY=your_key_here
