# Improving KITTYâ€™s Webâ€‘Search Reasoning Loop

## 1. Objective

KITTY sometimes repeats nearly identical **Thought** and **Action** steps, reâ€‘running the same `web_search` query until it hits the maximum reasoningâ€‘iteration limit. This guide explains how to:

- Make each reasoning step add genuinely new information.
- Avoid redundant tool calls and loops.
- Recognize when a question is unanswerable or needs approximation.
- Produce useful answers earlier, with clear caveats.

Patterns from ğŸ¤– [ReActâ€‘style prompting](https://www.google.com/search?q=ReAct+prompting+reasoning+and+acting+llm+agents) and ğŸ§  [toolâ€‘using LLM agents](https://www.google.com/search?q=tool+using+llm+agents+design+patterns) are adapted here for KITTY.

---

## 2. The Current Failure Mode

From the trace, KITTY:

1. Repeats the same interpretation of the question every step.
2. Calls `web_search` with the **exact same query** many times.
3. Never updates its **beliefs** or **plan** based on new observations.
4. Eventually hits **max iterations** and fails with no answer.

Example pattern (simplified):

```text
Thought: I need to search for "current adoption rates Ollama vs Llama.cpp".
Action: web_search("current adoption rates Ollama vs Llama.cpp")
Observation: <some results>

Thought: I need to search for "current adoption rates Ollama vs Llama.cpp".
Action: web_search("current adoption rates Ollama vs Llama.cpp")
...
```

This indicates:

- No explicit **state** across steps.
- No requirement that steps show **progress**.
- No notion of **answerability** (e.g., â€œthis exact metric doesnâ€™t existâ€).
- No **runtime guardrail** against duplicate tool calls.

The rest of this document describes concrete changes to fix those issues.

---

## 3. Design Principles

### 3.1. Statefulness

Each step should operate over an explicit **scratchpad**, not just freeâ€‘form text.

The scratchpad tracks:

- The **question** and its current **interpretation**.
- Any **assumptions** youâ€™re making.
- The **plan** (subâ€‘steps).
- A history of **tool calls** and **observations**.
- Collected **facts**.
- Current **uncertainties** or open subâ€‘questions.
- A **candidate answer** and its confidence.
- An **answerability status**.

### 3.2. Progressful Steps

Every reasoning step should:

1. Add new information **or**
2. Change the plan **or**
3. Move closer to a final answer.

If a step simply restates the same thought and proposes the same action, it should be rejected.

### 3.3. Search Diversification

Multiple web calls must not all be the same. Instead, they should explore **different angles**, e.g.:

- General overview
- Side A only (e.g., Ollama)
- Side B only (e.g., llama.cpp)
- Proxies (GitHub stats, downloads, enterprise case studies, etc.)

### 3.4. Answerability and Graceful Degradation

KITTY must regularly answer:

> Is the userâ€™s **exact** question answerable with available web data?

Possible answers:

- **(a)** Yes, directly.
- **(b)** Only approximately (via proxies).
- **(c)** Probably not (data doesnâ€™t exist or isnâ€™t public).

For (b) and (c), it should stop searching and:

- Explain what is and isnâ€™t available.
- Provide the best qualitative / proxyâ€‘based answer it can.

This is similar in spirit to ğŸ§ª [LLM selfâ€‘reflection](https://www.google.com/search?q=self+reflection+llm+agents+answerability) techniques.

---

## 4. Scratchpad Structure

You can model the scratchpad as a JSONâ€‘like object stored in memory for the task:

```json
{
  "question": "",
  "interpretation": "",
  "assumptions": [],
  "plan": [],
  "tool_calls": [],
  "facts": [],
  "uncertainties": [],
  "candidate_answer": null,
  "answerability": "unknown"
}
```

**Suggested fields:**

- `question`: Original user query.
- `interpretation`: KITTYâ€™s current paraphrase / understanding.
- `assumptions`: Explicit guesses (e.g., â€œâ€˜magnificent 7â€™ = big tech megaâ€‘capsâ€).
- `plan`: Highâ€‘level steps (2â€“4 bullets).
- `tool_calls`: List of `{tool, args, result_summary}`.
- `facts`: Normalized, deâ€‘duplicated facts extracted from observations.
- `uncertainties`: Important unknowns (e.g., â€œNo direct adoption metrics foundâ€).
- `candidate_answer`: Best current answer + rationale.
- `answerability`: `"unknown" | "direct" | "proxy_only" | "unlikely"`.

The orchestration layer can persist and pass this scratchpad to each reasoning step.

---

## 5. Perâ€‘Step Prompt Template (â€œDeltaâ€‘Onlyâ€)

To avoid repetition, each step should **only talk about what changed** since the previous step.

### 5.1. Template

You can embed this template in KITTYâ€™s internal prompt:

```text
You are at reasoning step {n} for the current question.

You are given a scratchpad with:
- question
- interpretation
- assumptions
- plan
- tool_calls
- facts
- uncertainties
- candidate_answer
- answerability

Your job at this step:

1. Briefly list only the NEW evidence since the last step (max 3 bullets).
2. Briefly list what remains uncertain (max 2 bullets).
3. Update the answerability status:
   - Choose one: (a) direct, (b) proxy_only, (c) unlikely.
4. Decide the next move:
   - If (b) or (c): DO NOT call web_search again; move toward a final answer.
   - If (a) and more data is needed:
       - Propose ONE new action that is materially different from prior actions
         (new query terms, a different site focus, or a summarization step).
5. Update the scratchpad accordingly.
6. If you believe you can now answer the question well enough, stop and draft the final answer instead of calling more tools.
```

At the end of each step, the model must output both:

- The updated scratchpad, and
- Either a tool call or a draft answer.

---

## 6. Webâ€‘Search Strategy for KITTY

### 6.1. Search Phases

Define simple phases for search:

1. **Phase 1 â€“ Broad scan**

   - 1â€“2 general queries.
   - Goal: identify main concepts, existing metrics, obvious limitations.

2. **Phase 2 â€“ Focused probes**

   - Separate queries for each key component:
     - Ollama adoption / ecosystem
     - llama.cpp adoption / ecosystem
   - Additional queries for **proxies**, such as:
     - GitHub stars / commits
     - Package downloads
     - Enterprise case studies / blog posts
   - Example: ğŸ“ˆ [Ollama GitHub stars and usage](https://www.google.com/search?q=Ollama+GitHub+stars+usage+statistics)  
     Example: ğŸ‘ [llama.cpp adoption GitHub stats](https://www.google.com/search?q=llama.cpp+adoption+github+stars+usage)

3. **Phase 3 â€“ Synthesis**

   - No more web calls.
   - Aggregate facts, confront uncertainties.
   - Produce final answer with clear caveats.

### 6.2. Query Diversification Rules

Add explicit rules for KITTY when calling `web_search`:

- Each new query must:
  - Differ by at least **3 meaningful tokens** from previous queries, and
  - Target a **different angle** (general vs proxies vs one side vs the other).

For example, instead of repeatedly calling:

```text
"current adoption rates Ollama vs Llama.cpp"
```

KITTY would evolve through:

```text
"ollama adoption stats github stars downloads"
"llama.cpp usage metrics github stars downloads"
"ollama vs llama.cpp production usage comparison"
"ollama enterprise case studies vs llama.cpp"
```

If none of these yield a quantitative â€œadoption rate,â€ KITTY should decide that **direct numbers are unavailable** and move to a qualitative comparison.

---

## 7. Orchestratorâ€‘Side Guardrails

You can catch many issues outside the model by adding simple checks in the orchestrator.

### 7.1. Duplicate Toolâ€‘Call Filter

Before executing a tool call, compare it to recent history:

```python
def should_execute_tool_call(tool_name, args, recent_calls, k=5):
    for call in recent_calls[-k:]:
        if call["tool_name"] == tool_name and call["args"] == args:
            return False  # duplicate
    return True
```

If `should_execute_tool_call` returns `False`, ask the model to:

- Change its query,
- Move to synthesis, or
- Declare the question unanswerable as asked.

This prevents the â€œstuck loopâ€ where the same query is used 10 times.

### 7.2. Reasoningâ€‘Step Budget with Early â€œBest Effortâ€

Instead of just a hard maxâ€‘step failure, define:

- A **soft budget** (e.g., 3â€“5 steps) after which the model is encouraged to answer.
- A **hard budget** (e.g., 8â€“10 steps) at which the model must:
  - Use whatever it has gathered, and
  - Return a bestâ€‘effort answer that explicitly explains limitations.

Sketch:

```python
if step >= soft_budget:
    # Nudge toward answering
    system_msg = (
        "You have already used several reasoning steps. "
        "Prefer synthesizing and answering now using what you have."
    )

if step >= hard_budget:
    # Force answering instead of another tool call
    system_msg = (
        "You have reached the maximum reasoning steps. "
        "Do NOT call tools again. Synthesize the best possible answer "
        "from available facts, and clearly state any missing data."
    )
```

---

## 8. Handling Ambiguous / Illâ€‘Posed Questions

For queries like:

> â€œCurrent rate of adoption for Ollama vs llama.cpp in the magnificent 7â€

KITTY should:

1. **Identify ambiguity**

   - â€œmagnificent 7â€ is unclear:
     - S&P â€œMagnificent 7â€ megaâ€‘cap stocks?
     - Some internal group of 7 companies?
     - Something else entirely?

2. **Check if a metric exists**

   - There is no public, standardized â€œadoption rate of tools X and Y within group Zâ€ metric.
   - Proxies will have to be used.

3. **Respond explicitly**

   Rather than forcing more search, KITTY should say:

   - It couldnâ€™t find any direct â€œadoption rateâ€ data for that specific cohort.
   - It can, however, compare:
     - Overall popularity,
     - GitHub stats,
     - Blog posts and production use reports, etc.
   - Then provide a **qualitative** comparison backed by the data it *does* have.

Encourage this behavior via instructions similar to:

```text
If you cannot find direct numerical data after 2â€“3 distinct web_search calls,
you MUST stop searching for that exact number.

Instead, use proxies (GitHub activity, downloads, case studies, etc.) to
form a qualitative or approximate answer, and clearly state that the exact
metric is unavailable.
```

This aligns with best practices for ğŸ•µï¸ [handling ambiguous queries](https://www.google.com/search?q=handling+ambiguous+user+queries+in+chatbots).

---

## 9. Example: Rewritten Flow for the Ollama vs llama.cpp Query

Below is a sketch of how KITTY should behave after the changes.

### Step 0 â€“ Initialization

- Interpretation:
  - Compare adoption of **Ollama** vs **llama.cpp**.
  - â€œMagnificent 7â€ is ambiguous â†’ assume it refers to major tech firms unless otherwise specified.
- Plan:
  1. Broad scan for any published adoption metrics or surveys.
  2. Focused search for proxies (GitHub stats, blog posts, case studies).
  3. Synthesize qualitative comparison and note ambiguity.

### Step 1 â€“ Broad Scan

- Query: `"ollama vs llama.cpp adoption comparison"`
- Observation: articles comparing features and usage contexts but no numeric â€œadoption rateâ€.
- Facts:
  - Both are widely used for local LLM inference.
  - Ollama focuses on easy local use; llama.cpp is a lowerâ€‘level inference engine.

Update:

- Uncertainty: no direct adoption metrics yet.
- Answerability: `proxy_only`.

### Step 2 â€“ Focused Proxies (Ollama)

- Query: `"Ollama GitHub stars downloads active users"`
- Collect:
  - GitHub stars, release activity, mentions in blogs, etc.

### Step 3 â€“ Focused Proxies (llama.cpp)

- Query: `"llama.cpp GitHub stars usage examples production"`
- Collect:
  - GitHub stars, forks, ecosystem notes.

### Step 4 â€“ Synthesis

- No more `web_search`.
- Compare:
  - Age of projects.
  - Ecosystem maturity.
  - Typical use cases.
- Final answer:
  - Explain absence of precise â€œadoption rateâ€.
  - Provide a reasoned qualitative comparison.
  - State assumptions about â€œmagnificent 7â€.

This is a far more informative outcome than â€œMaximum reasoning iterations reached without finding an answer.â€

---

## 10. Rollout Checklist

Use this as a quick checklist when you implement and tune KITTY:

1. **Scratchpad**
   - [ ] Added a structured scratchpad object.
   - [ ] Each step reads and updates the scratchpad.

2. **Perâ€‘Step Template**
   - [ ] Steps are â€œdeltaâ€‘onlyâ€ (new evidence + changed uncertainties).
   - [ ] Answerability status is updated regularly.

3. **Search Behavior**
   - [ ] Queries are diversified by design (different angles).
   - [ ] Thereâ€™s a clear pivot to proxies if direct data is missing.

4. **Orchestrator Guardrails**
   - [ ] Duplicate tool calls with identical args are blocked.
   - [ ] A soft budget nudges synthesis.
   - [ ] A hard budget forbids further tool calls and forces a bestâ€‘effort answer.

5. **Ambiguity Handling**
   - [ ] Ambiguous terms (e.g., â€œmagnificent 7â€) are explicitly identified.
   - [ ] KITTY either states its assumption or asks for clarification (if the environment permits).
   - [ ] Nonâ€‘existent metrics result in a clear explanation plus a proxyâ€‘based answer, not an infinite search loop.

By combining these changes, KITTYâ€™s webâ€‘search reasoning loop becomes:

- More **efficient** (fewer wasted steps),
- More **robust** (less likely to get stuck),
- More **transparent** (clear about what is and isnâ€™t knowable), and
- More **useful** to the user, even when the â€œperfectâ€ data simply doesnâ€™t exist.
